{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hi! \u270c\ufe0f","text":"<p>We like to write about developing enterprise software that utilizes LLMs, open-source projects, and practical applications of LLMs. We share our experiences and thoughts on making AI systems robust for enterprise use.</p>"},{"location":"#who-we-are","title":"Who we are","text":"<p>We're AI consultants and researchers specializing in developing enterprise-ready LLM systems. Our focus is on creating reliable, scalable AI solutions for real-world applications.</p>"},{"location":"#need-ai-expertise","title":"Need AI expertise?","text":"<p>If you're looking to implement or improve AI systems in your organization or if you just want to chat about AI, reach out!</p>"},{"location":"contact/","title":"Get In Touch","text":"<p>If you have any questions or want to get in touch, you can fill out this form!</p>"},{"location":"blog/","title":"Blog","text":"<p>Some light reading on AI. If you are looking for more technical content, check out the experiments section.</p>"},{"location":"blog/2024/11/10/handing-over-my-wallet-to-ai-which-model-gave-the-best-financial-advice/","title":"Handing Over My Wallet to AI: Which Model Gave the Best Financial Advice?","text":"<p>Ever looked at your bank account and thought \"I should probably talk to a personal financial advisor\" \u2014 but then remembered that good advisors charge anywhere from $150 to $300 per hour? For most of us, professional financial advice feels like a luxury we can't justify. But we shouldn't have to wait until we're rich to get good financial advice!</p> <p>That's where AI might change everything. Instead of paying hundreds per hour for financial advice, what if you could get personalized insights for the cost of a ChatGPT Plus subscription? To test this possibility, I connected RocketMoney to all my accounts\u2014checking, credit cards, investments, the works\u2014and exported 90 days of transaction data. Then I fed this financial snapshot to three AI heavyweights: ChatGPT, Claude, and Gemini.</p> <p>But this isn't just about which AI is \"smarter.\" Each platform brings different tools and features that the model can use to analyze the data. I asked each to analyze my spending and create a comprehensive financial plans and reports, just like a human advisor would.</p> <p>I kept it simple. Each AI received the same prompt:</p> <p>You are an expert personal finance manager and wealth advisor. I have included my last 90 days of transactions. I need you to do an analysis of my current financial situation and give me a report and wealth plan. Keep in mind this csv is a consolidation for all of my accounts and includes transfers and credit card payments provided by RocketMoney.</p> <p>The results? Let's just say one AI saved me more money in potential insights than a year's worth of its subscription costs\u2014while another couldn't even handle the basics. Here's what happened when I turned my finances over to the machines.</p>"},{"location":"blog/2024/11/10/handing-over-my-wallet-to-ai-which-model-gave-the-best-financial-advice/#chatgpt-a-mixed-bag-of-analysis","title":"ChatGPT: A Mixed Bag of Analysis","text":"<p>Everybody knows ChatGPT already, so I'll skip the introduction. ChatGPT allows you to upload files, but their strongest model (GPT-o1) doesn't support file uploads yet so I opted to use GPT-4o as my model of choice.</p> <p>The analysis began promisingly. ChatGPT used python to quickly parse my CSV file and showed me its understanding of the data structure by listing the columns.</p> <p>It then produced a \"Financial Summary\" with my total income, expenses, and net balance. The numbers were significantly off base\u2014likely because it counted both credit card charges and their payments as separate expenses.</p> <p>Next came a breakdown of spending by category, presented in an interactive spreadsheet. The spreadsheet even had an Expand feature for a full-screen view that even let you select and reference specific cells:</p> <p></p> <p>I do love the way ChatGPT presents the data in a spreadsheet-like format. It's easy to see the data and it's easy to interact with. The data wasn't particularly useful since I already have RocketMoney for category breakdowns, plus it includes credit card payments and transfers. ChatGPT wrapped up with some suggestions for next steps:</p> <p>I've provided you with a summary of your financial situation over the last 90 days, including total income, expenses, net balance, and a detailed breakdown of spending by category.</p> <p>Here's what I'll do next for your wealth plan:</p> <ol> <li> <p>Spending Analysis:</p> <ul> <li>Dive deeper into your largest expense categories and provide insights on where you could cut back to save more.</li> </ul> </li> <li> <p>Savings and Investment Strategy:</p> <ul> <li>Based on your net balance, I'll suggest a strategy for increasing savings or investments to help grow your wealth.</li> </ul> </li> <li> <p>Debt Management:</p> <ul> <li>I'll assess any debt-related payments, such as credit card expenses, to determine the best approach to reduce interest payments and manage liabilities effectively.</li> </ul> </li> </ol> <p>Would you like me to focus on any specific area first, or do you have particular goals (e.g., saving for a big purchase, paying down debt, growing investments) in mind for your wealth plan?</p> <p>Wanting to dig deeper, I prompted:</p> <p>\"Where should I start to reduce my spending? I spend too much!\"</p> <p>Without doing any further python analysis, ChatGPT jumped straight into suggestions\u2014and this is where things got interesting, and not in a good way.</p> <p>First, it identified my biggest expense category as \"Credit Card Payments,\" which shows a fundamental misunderstanding\u2014credit card payments aren't really an expense category, they're just transfers to pay down credit balances. It was essentially double-counting expenses: once when I used the credit card, and again when I paid the bill.</p> <p>Next, it focused on bills and utilities as my second-highest expense category. While technically true, the advice was comically generic\u2014suggesting I switch to LED lightbulbs and turn off appliances when leaving rooms. Not exactly the sophisticated financial analysis I was hoping for!</p> <p>Then came the food spending advice:</p> <p>The advice I received was surprisingly generic. ChatGPT seems to deliberately limit the amount of data it processes (maybe to conserve tokens?) which leads to surface-level analysis and cookie-cutter suggestions like:</p> <ol> <li>Focusing on \"Credit Card Payments\" as an expense category (which isn't really an expense at all)</li> <li>Suggesting I switch to LED lightbulbs and turn off appliances to reduce my bills spending</li> <li>Making generic food spending suggestions without any reference to my actual dining patterns</li> </ol> <p>ChatGPT didn't dig deep enough to notice that my real issue was excessive UberEats spending and fancy restaurant visits. Instead, it gave blanket advice about cooking at home more\u2014true, but not particularly insightful given the data available.</p>"},{"location":"blog/2024/11/10/handing-over-my-wallet-to-ai-which-model-gave-the-best-financial-advice/#3-dining-drinks-xxxx","title":"3. Dining &amp; Drinks - $XXXX","text":"<ul> <li>What's happening: This is one of the most common areas where expenses accumulate because of convenience.</li> <li>What to do:<ul> <li>Cut Back on Eating Out: Allocate a fixed budget for eating out each week. Consider cooking at home or meal prepping more often, especially given your goal of saving by meal prepping.</li> <li>Plan Meals: Develop simple meal plans with affordable ingredients, focusing on dishes that offer variety yet are easy to prepare. This can help avoid impulse restaurant spending.</li> <li>Limit High-Cost Visits: Target reducing high-cost visits (e.g., fine dining) and focus on affordable alternatives like picnics or home-prepared meals.</li> </ul> </li> </ul>"},{"location":"blog/2024/11/10/handing-over-my-wallet-to-ai-which-model-gave-the-best-financial-advice/#gemini-a-comedy-of-errors","title":"Gemini: A Comedy of Errors","text":"<p>I had high hopes for Gemini Advanced, given Google's claims about its enhanced analytical capabilities and extremely large context window. Those hopes were quickly dashed.</p> <p></p> <p>My first attempt resulted in Gemini apologizing and refusing to help\u2014a strange response given that financial analysis is hardly controversial. On my second try, it analyzed the columns without any error but went off on a bizarre tangent about tax-deductible expenses (which I hadn't asked about) and attempted to analyze a non-existent data column named \"Tax Deductible\", which obviously failed.</p> <p></p> <p>This experience was very disappointing. I am still bullish on Gemini's potential, but it seems completely unusable for this kind of task at its current state.</p>"},{"location":"blog/2024/11/10/handing-over-my-wallet-to-ai-which-model-gave-the-best-financial-advice/#claude-the-financial-advisor-we-need","title":"Claude: The Financial Advisor We Need","text":"<p>Where the others stumbled, Claude shined. Using Claude 3.5 Sonnet in a dedicated project workspace, I got what felt like genuinely professional financial advice.</p> <p>\ud83d\udea8 Be sure to enable the \"Analysis Tool\" in the beta features if you want to try this out yourself.</p> <p>What set Claude apart was its comprehensive approach. It started with a thorough financial report that broke down both total income and expenses. While it did make some mistakes (like double-counting my income due to a recent job change), it showed remarkable attention to detail in its analysis.</p> <p>Next, it provided a detailed spending breakdown, avoiding the common pitfall of counting transfers and credit card payments as expenses. It provided clear percentage breakdowns\u2014for instance, noting that bills made up 28% of my expenses and food was 13%, further split between groceries and dining out.</p> <p>Here's when Claude started to impress me\u2014in the details:</p> <p>Spending Patterns:</p> <ul> <li>Essential Spending: $XXXX (58%)</li> <li>Discretionary Spending: $XXXX (42%)</li> <li>Heavy reliance on food delivery services (frequent Uber Eats charges)</li> <li>Multiple subscription services totaling approximately $XXX/month</li> </ul> <p>Claude understood what was essential and what was discretionary, which shows a level of understanding that goes beyond the surface-level categories. It immediately called out my frequent Uber Eats charges and doesn't include credit card payments or transfers as in the spending breakdown.</p> <p>It even called out specific \"Red Flags\":</p> <p>RED FLAGS:</p> <ul> <li>High Food Costs: Spending $XXXX/month on food, with about 67% on dining out/delivery</li> <li>Numerous Subscriptions: Multiple overlapping services (Apple, Claude, OpenAI, etc.)</li> <li>High discretionary spending ratio</li> </ul> <p>It is not wrong here, I do sign up for every AI product that I hear about. Busted!</p> <p>Claude then provided a detailed action plan:</p> <p>I then asked for visualizations, and Claude delivered with beautiful charts showing my spending patterns:</p> <p></p> <p>I then challenged Claude to show me potential savings from cutting out UberEats and limiting dining out. It responded with a detailed plan and an interactive visualization that you can check out here!</p> <p>I followed up with a few more requests:</p> <ul> <li>Identifying other areas for spending cuts</li> <li>Configuring automatic savings and investments</li> <li>Analyzing the rent vs. buy decision</li> </ul> <p>Each response was thoughtful, data-driven, and genuinely helpful! By the time I hit my messaging limits, I had a comprehensive financial plan and a clear path forward, which I saved to the Claude project so I can reference it whenever I want.</p>"},{"location":"blog/2024/11/10/handing-over-my-wallet-to-ai-which-model-gave-the-best-financial-advice/#recommendations","title":"RECOMMENDATIONS:","text":""},{"location":"blog/2024/11/10/handing-over-my-wallet-to-ai-which-model-gave-the-best-financial-advice/#immediate-actions","title":"Immediate Actions:","text":"<ul> <li> <p>Audit Subscriptions:</p> <ul> <li>Consolidate AI services (currently using Claude, OpenAI, and others)</li> <li>Review multiple Apple subscriptions</li> <li>Evaluate necessity of multiple cloud services</li> </ul> </li> <li> <p>Food Budget Optimization:</p> <ul> <li>Reduce Uber Eats usage (currently spending heavily here)</li> <li>Increase grocery shopping vs. delivery</li> <li>Target reducing food expenses to $XXXX/month</li> </ul> </li> </ul>"},{"location":"blog/2024/11/10/handing-over-my-wallet-to-ai-which-model-gave-the-best-financial-advice/#medium-term-financial-goals","title":"Medium-term Financial Goals:","text":"<ul> <li> <p>Build Emergency Fund:</p> <ul> <li>Target: 6 months of essential expenses ($XXXX)</li> <li>Current high income provides good opportunity for savings</li> </ul> </li> <li> <p>Implement 50/30/20 Budget:</p> <ul> <li>50% needs ($XXXX)</li> <li>30% wants ($XXXX)</li> <li>20% savings ($XXXX)</li> </ul> </li> </ul>"},{"location":"blog/2024/11/10/handing-over-my-wallet-to-ai-which-model-gave-the-best-financial-advice/#investment-strategy","title":"Investment Strategy:","text":"<ul> <li> <p>Maximize Tax-Advantaged Accounts:</p> <ul> <li>401(k) if available through employer</li> <li>Roth IRA ($XXXX/year)</li> <li>HSA if eligible</li> </ul> </li> <li> <p>Consider diversifying investments:</p> <ul> <li>Currently see some crypto investments</li> <li>Recommend broader diversification into index funds</li> <li>Consider real estate investment given high income</li> </ul> </li> </ul>"},{"location":"blog/2024/11/10/handing-over-my-wallet-to-ai-which-model-gave-the-best-financial-advice/#what-this-means-for-the-future-of-ai-financial-advice","title":"What This Means for the Future of AI Financial Advice","text":"<p>This experiment revealed both the potential and current limitations of AI in personal finance:</p> <ol> <li> <p>Data Processing Matters: The ability to correctly interpret financial data\u2014including understanding what to ignore\u2014is crucial. This is where Claude excelled and others faltered.</p> </li> <li> <p>Context is King: Generic financial advice is everywhere. What's valuable is AI that can understand your specific situation and habits.</p> </li> <li> <p>Visualization + Analysis: The most helpful insights came when AI could both crunch the numbers AND present them in an intuitive way.</p> </li> </ol>"},{"location":"blog/2024/11/10/handing-over-my-wallet-to-ai-which-model-gave-the-best-financial-advice/#the-bottom-line","title":"The Bottom Line","text":"<p>While all three AI models showed promise, only Claude provided the kind of comprehensive, personalized financial analysis that could genuinely help someone improve their financial health. It's a glimpse of what's possible when AI moves beyond simple number-crunching to deliver truly personalized financial guidance.</p> <p>Does this mean AI is ready to replace human financial advisors? Not quite. But it suggests that AI could become an invaluable tool for both individuals managing their finances and professionals looking to provide better service to their clients.</p> <p>For now, if you're considering using AI for financial analysis, my experiment suggests Claude is your best bet\u2014just don't expect it to make the hard decisions for you. After all, choosing between that fancy dinner and your savings goal is still up to you.</p>"},{"location":"blog/2024/08/30/should-you-even-trust-geminis-million-token-context-window/","title":"Should You Even Trust Gemini\u2019s Million-Token Context Window?","text":"<p>\ud83d\udcd6 Read On Medium</p> <p>Imagine you\u2019re tasked with analyzing your company\u2019s entire database \u2014 millions of customer interactions, years of financial data, and countless product reviews \u2014 to extract meaningful insights. You turn to AI for help. You shove all of the data into Google Gemini 1.5, with its new 1 million token context length and start making requests, which it seems to be solving. But a nagging question persists: Can you trust the AI to accurately process and understand all of this information? How confident can you be in its analysis when it\u2019s dealing with such a vast amount of data? Are you going to have to dig through a million tokens worth of data to validate each answer?</p> <p>Traditional AI tests, like the well-known \u201cneedle-in-a-haystack\u201d tests, fall short in truly assessing an AI\u2019s ability to reason across large, cohesive bodies of information. These tests often involve hiding unrelated information (needles) in an otherwise homogeneous context (haystack). The problem is that it makes the focus on information retrieval and anomaly detection rather than comprehensive understanding and synthesis. Our goal wasn\u2019t just to see if it could find a needle in a haystack, but to evaluate if it could understand the entire haystack itself.</p> <p>Using a real-world dataset of App Store information, we systematically tested Gemini 1.5 Flash across increasing context lengths. We asked it to compare app prices, recall specific privacy policy details, and evaluate app ratings \u2014 tasks that required both information retrieval and reasoning capabilities. For our evaluation platform, we used LangSmith by LangChain, which proved to be an invaluable tool in this experiment.</p> <p>The results were nothing short of amazing! Lets dive in.</p>"},{"location":"blog/2024/08/30/should-you-even-trust-geminis-million-token-context-window/#setting-up-the-experiment","title":"Setting Up the Experiment","text":"<p>You can follow along with our full experiment in this Jupyter notebook.</p>"},{"location":"blog/2024/08/30/should-you-even-trust-geminis-million-token-context-window/#choosing-our-datasets","title":"Choosing Our Datasets","text":"<p>We need 3 datasets for our experiment:</p> <ol> <li>App Data Dataset: We used the Apple App Store Data Set, a real-world collection of information about 10,000 apps. This dataset provided rich, varied information for us to analyze.</li> <li>Golden Dataset: We selected 5 apps arbitrarily that we will use to craft our Evaluation Dataset questions and ground truth answers. These 5 apps will need to be included in the context in every step of the experiment.</li> <li>Evaluation Dataset: We crafted a set of three questions and answers based on the Golden Dataset. These are the questions we will ask Gemini Flash and we will evaluate it\u2019s answer against the ground truth answer we have written.</li> </ol> <pre><code>examples = [\n    {\n        \"question\": \"Do the 'Sago Mini Superhero' and 'Disney Channel  Watch Full Episodes Movies  TV' apps require internet connection?\",\n        \"answer\": \"You can play Sago Mini Superhero without wi-fi or internet. Internet is required for Disney Channel  Watch Full Episodes Movies  TV\"\n    },\n    {\n        \"question\": \"Where can I find the privacy policy for the 'Disney Channel  Watch Full Episodes Movies  TV' app?\",\n        \"answer\": \"http://disneyprivacycenter.com/\"\n    },\n    {\n        \"question\": \"Which one costs less? The 'KQ MiniSynth' app or the 'Sago Mini Superhero' app?\",\n        \"answer\": \"The 'KQ MiniSynth' app costs $5.99, the 'Sago Mini Superhero' app costs $2.99. So 'Sago Mini Superhero' is cheaper\"\n    }\n]\n</code></pre>"},{"location":"blog/2024/08/30/should-you-even-trust-geminis-million-token-context-window/#leveraging-gemini-15-flash","title":"Leveraging Gemini 1.5 Flash","text":"<p>For our AI model, we utilized Google\u2019s Gemini 1.5 Flash. This model allows up to 1 million tokens in it\u2019s context window, which is roughly 700,000 words! At the time of writing this, Gemini 1.5 Flash costs ~$0.70/million input tokens, and thats without caching. That is comparable to GPT-3.5 or Claude Haiku pricing.</p>"},{"location":"blog/2024/08/30/should-you-even-trust-geminis-million-token-context-window/#langsmith-our-evaluation-platform","title":"LangSmith: Our Evaluation Platform","text":"<p>For managing our experiment and evaluating results, we turned to LangSmith by LangChain. LangSmith offers gives us access to a few features that are perfect for this kind of experiment:</p> <ol> <li>When we upload our evaluation dataset to LangSmith, we can version control, split, and even generate synthetic data, all from the LangSmith console.</li> <li>All of our experimentation results and traces are tracked in LangSmith for every dataset. The dashboard allows us to visualize the performance across different context lengths.</li> </ol> <p>When running our experiment, it is not required but it is quite convenient to use automatic evaluations using an LLM as a judge. In our case that means for each question in our dataset:</p> <ol> <li>Gemini Flash takes a whack at answering the question</li> <li>We have GPT-4o score if Flash\u2019s answer is correct based on the ground truth answer we have written in the dataset.</li> </ol> <p>This functionality happens through LangSmith Custom Evaluators, which are simple python functions that return a score for the evaluation:</p> <pre><code># We define the scoring schema for the LLM to respond in\n# using Pydantic\nclass EvaluationSchema(BaseModel):\n    \"\"\"An evaluation schema for assessing the correctness of an answer\"\"\"\n    reasoning: str = Field(\n        description=\"Detailed reasoning for the evaluation score\")\n    correct: bool = Field(\n        description=\"Whether the user's answer is correct or not\")\n\n# Our evaluation function\ndef qa_eval(root_run: Run, example: Example):\n    \"\"\"Evaluate the correctness of an answer to a given question\"\"\"\n    # The question from the dataset example\n    question = example.inputs[\"question\"]\n\n    # Gemini's answer\n    flash_answer = root_run.outputs[\"output\"]\n\n    # Ground truth answer from the dataset\n    correct_answer = example.outputs[\"answer\"]\n\n    # Force GPT-4o to respond in the scoring schema\n    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.4).with_structured_output(EvaluationSchema)\n\n    system_prompt = f\"\"\"You are a judge tasked with evaluating a user's answer to a given question.\nYou will be provided with the question, the correct answer, and the user's thought process and answer.\n\nQuestion:\n{question}\n\nCorrect Answer:\n{correct_answer}\n\nYour job is to assess the user's answer and provide:\n1. Detailed reasoning for your evaluation, comparing the user's answer to the correct answer\n2. A boolean judgment on whether the user's answer is correct or not\n\nBe thorough in your reasoning and accurate in your judgment. Consider partial correctness and any nuances in the answers.\"\"\"\n\n    # Invoke the model with all of the context\n    evaluation: EvaluationSchema = llm.invoke(\n        [SystemMessage(content=system_prompt),\n         HumanMessage(content=flash_answer)]\n    )\n\n    score = 1 if evaluation.correct else 0\n\n    return {\n        \"score\": score,\n        \"key\": \"correctness\",\n        \"comment\": evaluation.reasoning\n    }\n</code></pre> <p>If you\u2019re not familiar with LangChain or Python, we are simply writing a function (<code>qa_eval</code>) that takes the question from the dataset, Flash\u2019s answer, and the correct answer and putting them all into a prompt for GPT-4o. We use <code>.with_structured_output</code> to ensure that the LLM responds in a specific schema that we can use to return the score in the schema that LangSmith expects.</p>"},{"location":"blog/2024/08/30/should-you-even-trust-geminis-million-token-context-window/#running-the-experiment","title":"Running the Experiment","text":"<p>We gradually increased the context length up to the full million-token capacity in 50,000 token increments. To generate these varying context lengths, we wrote a function <code>get_context</code> that would:</p> <ol> <li>Start with our \u201cgolden dataset\u201d of 5 apps</li> <li>Add additional app data until we reached the desired token count</li> <li>Randomize the order of apps in the context to avoid any positional bias</li> </ol>"},{"location":"blog/2024/08/30/should-you-even-trust-geminis-million-token-context-window/#target-function","title":"Target Function","text":"<p>We use this <code>get_context</code> function in our \u201ctarget function\u201d. The target function describes the function that will be used to produce the output that needs to be evaluated. In our case the target function:</p> <ol> <li>Fills up the context window with app data up to the number of tokens we are testing in that step</li> <li>Puts the context into a prompt for Gemini Flash</li> <li>Queries Gemini Flash with the question from the dataset and returns the model\u2019s response</li> </ol> <p>Here\u2019s a simplified version of our target function:</p> <pre><code>def predict(inputs: dict):\n    tokens = (max_context_limit / total_steps) * current_step\n    context = get_context(tokens)\n\n    system_prompt = f\"\"\"You are tasked with answering user questions based on the App Store data inside &lt;APP STORE DATA&gt;.\n    Use ONLY the information provided in the context. Be as accurate as possible.\"\"\"\n\n    response = llm.invoke([\n        SystemMessage(content=system_prompt),\n        HumanMessage(content=inputs[\"question\"])\n    ])\n\n    return {\"output\": response.content}\n</code></pre> <p>We wrap all of this up into a custom Python class just to make it easy to keep track of the steps and control each experiment and we are ready to run!</p> <pre><code>result = evaluate(\n    self.predict, # Our predict function\n    data=client.list_examples(dataset_name=dataset_name), # Our evaluation dataset\n    evaluators=[qa_eval], # Our custom evaluator\n    experiment_prefix=f\"{self.model}-{tokens}\" # Prefixes the experiments in Langsmith for readability\n)\n</code></pre>"},{"location":"blog/2024/08/30/should-you-even-trust-geminis-million-token-context-window/#results","title":"Results","text":"<p>The results of our experiment with Gemini 1.5 Flash were nothing short of amazing! Across all context lengths, from 50,000 tokens all the way up to the full million-token capacity, Gemini 1.5 Flash achieved 100% accuracy in answering our test questions!</p> <p>Experiment Results in LangSmith</p> <p>You can view the full test results on LangSmith here.</p> <p>Let\u2019s break down what this means:</p> <ol> <li>\ud83d\udd2c Perfect Accuracy: Regardless of whether we gave Gemini 1.5 Flash 50,000 tokens of context or 1,000,000 tokens, it consistently provided correct answers to our questions. This level of consistency is impressive and suggests that the model can effectively process and utilize information from extremely large contexts.</li> <li>\ud83e\udde0 Information Synthesis: Our questions required more than just information retrieval. They involved comparing data points from different parts of the context. Gemini 1.5 Flash\u2019s perfect score indicates a strong ability to understand information across a huge context, not just locate specific strings.</li> </ol> <p>To put this in perspective, at the maximum context length, Gemini 1.5 Flash was accurately answering questions while processing the equivalent of a 400-page book in a single query. This is a significant leap beyond traditional document analysis capabilities.</p> <p>However, it\u2019s important to note some limitations of our experiment:</p> <ol> <li>Question Complexity: Our questions, while requiring synthesis, are relatively straightforward. We deliberately avoided questions requiring complex numerical reasoning or identifying trends across the entire dataset.</li> <li>Limited Question Set: We used a small set of questions for each evaluation. A larger, more diverse set of questions could provide even more robust insights into the model\u2019s capabilities.</li> </ol> <p>Despite these limitations, the results are extremely promising. They suggest that Gemini 1.5 Flash can maintain high accuracy and information synthesis capabilities with very low costs, even when dealing with massive amounts of context. This opens up exciting possibilities for applications in various fields, from business analytics to scientific research.</p>"},{"location":"experiments/","title":"Experiments","text":"<p>Welcome to our experiments section! Here you'll find various experiments we've conducted to explore different aspects of AI and machine learning in the form of Jupyter Notebooks.</p>"},{"location":"experiments/#available-experiments","title":"Available Experiments","text":"<ul> <li>Context Length Experiment: An investigation into the effects of context length on AI model performance.</li> </ul> <p>Feel free to explore these experiments and learn from our findings!</p>"},{"location":"experiments/Context_Length_AppStoreV2/","title":"Context Length Experiment","text":"<p>You will need the following to recreate this experiment:</p> <ol> <li>A Langsmith account and API Key</li> <li>A Google AI Studio API key</li> <li>An OpenAI API key</li> </ol> <p>Create a copy of the .env.sample file saved as <code>.env</code> and add your API keys.</p> <p>Install the necessary libraries:</p> In\u00a0[\u00a0]: Copied! <pre>%pip install -qU pandas tiktoken langchain langchain-openai langchain-google-genai matplotlib langsmith python-dotenv seaborn\n</pre> %pip install -qU pandas tiktoken langchain langchain-openai langchain-google-genai matplotlib langsmith python-dotenv seaborn <p>Our experiment utilizes the App Store Apple Data Set (10k apps) from Kaggle. This dataset was chosen for its rich information about various apps, allowing for deterministic question generation and answer validation using pandas operations.</p> <p>We start by loading the necessary libraries and the two main CSV files from the dataset:</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom langsmith import Client\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = Client()\n\napp_data_df = pd.read_csv('./data/AppleStore.csv')\ndescriptions_df = pd.read_csv('./data/appleStore_description.csv')\n</pre> import pandas as pd from langsmith import Client from dotenv import load_dotenv  load_dotenv()  client = Client()  app_data_df = pd.read_csv('./data/AppleStore.csv') descriptions_df = pd.read_csv('./data/appleStore_description.csv') <p>The data is then merged into a single dataframe:</p> In\u00a0[3]: Copied! <pre>full_app_df = pd.merge(app_data_df, descriptions_df, on='id', how='left')\n</pre> full_app_df = pd.merge(app_data_df, descriptions_df, on='id', how='left') In\u00a0[4]: Copied! <pre>new_df = full_app_df[['id', 'track_name_x', 'size_bytes_x', 'currency',\n                      'price', 'rating_count_tot', 'user_rating', 'ver', 'prime_genre', 'app_desc']]\n\nnew_df = new_df.rename(columns={'track_name_x': 'name', 'size_bytes_x': 'size'})\n\nnew_df['size'] = new_df['size'] / (1024 * 1024)  # Convert to MB\nnew_df['name'] = new_df['name'].str.replace(r\"[^a-zA-Z0-9\\s]+\", \"\", regex=True)\nnew_df = new_df[new_df['name'].str.strip() != \"\"]\nnew_df = new_df[new_df['rating_count_tot'] != 0]\nnew_df = new_df[new_df['app_desc'].str.contains(r'[\\u4e00-\\u9fff]') == False]\nnew_df = new_df.sort_values(by='app_desc')\nnew_df.head()\n</pre> new_df = full_app_df[['id', 'track_name_x', 'size_bytes_x', 'currency',                       'price', 'rating_count_tot', 'user_rating', 'ver', 'prime_genre', 'app_desc']]  new_df = new_df.rename(columns={'track_name_x': 'name', 'size_bytes_x': 'size'})  new_df['size'] = new_df['size'] / (1024 * 1024)  # Convert to MB new_df['name'] = new_df['name'].str.replace(r\"[^a-zA-Z0-9\\s]+\", \"\", regex=True) new_df = new_df[new_df['name'].str.strip() != \"\"] new_df = new_df[new_df['rating_count_tot'] != 0] new_df = new_df[new_df['app_desc'].str.contains(r'[\\u4e00-\\u9fff]') == False] new_df = new_df.sort_values(by='app_desc') new_df.head() Out[4]: id name size currency price rating_count_tot user_rating ver prime_genre app_desc 6548 1134867821 NOT ALONE  Story of a bird 116.121094 USD 2.99 1 3.0 1.1 Games ! Now on X'mas special sales (~2017 Jan. 3rd) ... 6751 1145500015 Drifty Chase 180.987305 USD 0.00 1631 4.5 1.7 Games !! 2016 Very Big Indie Pitch finalist at PGCon... 2493 823804745 Multiplayer Terraria edition 15.058594 USD 3.99 6981 4.0 1.5 Games !!! First and the only app which allows to pla... 3273 949876643 Lumyer  augmented reality camera effects 116.251953 USD 0.00 3896 4.5 4.0.1 Photo &amp; Video !!! NEW !!!  TAP EFFECTS\\nTry the new Tap Effe... 5519 1086929344 Dancing with the Stars The Official Game 334.543945 USD 0.00 1098 4.0 2.7 Games !!! Please note this app does not currently su... In\u00a0[5]: Copied! <pre>golden_df = new_df[new_df['app_desc'].str.startswith(('K', 'k'))].head(5)\nclean_df = new_df.drop(golden_df.index)\ngolden_df\n</pre> golden_df = new_df[new_df['app_desc'].str.startswith(('K', 'k'))].head(5) clean_df = new_df.drop(golden_df.index) golden_df Out[5]: id name size currency price rating_count_tot user_rating ver prime_genre app_desc 1350 529997671 Disney Channel  Watch Full Episodes Movies  TV 125.921875 USD 0.00 21082 3.5 5.7.0 Entertainment K.C. Undercover, Liv &amp; Maddie, Bunk\u2019d and more... 3455 965789238 1000 73.989258 USD 0.00 23 4.5 3.6.5 Shopping KAOLA.COM is China 's largest overseas commodi... 3793 994674676 Sago Mini Superhero 171.169922 USD 2.99 30 3.5 1.1 Education KAPOW! Jack the rabbit bursts into the sky as ... 1965 645949180 Jelly Splash 132.311523 USD 0.00 21601 4.0 3.13.0 Games KICK BACK AND SPLASH!\\n\\nJoin those delicious ... 5076 1070850573 KQ MiniSynth 19.365234 USD 5.99 15 5.0 1.7.4 Music KQ MiniSynth is a polyphonic modular synthesiz... <p>This method selects the first five apps whose descriptions start with 'K' or 'k'. While somewhat arbitrary, this approach ensures:</p> <ol> <li>Reproducibility across experiments</li> <li>A degree of randomness to avoid bias</li> </ol> <p>By separating our golden dataset from the main dataframe, we can ensure these apps are always included in our context, regardless of the token limit, while filling the remaining context with other app data.</p> In\u00a0[6]: Copied! <pre>examples = [\n    {\n        \"question\": \"Do the 'Sago Mini Superhero' and 'Disney Channel  Watch Full Episodes Movies  TV' apps require internet connection?\",\n        \"answer\": \"You can play Sago Mini Superhero without wi-fi or internet. Internet is required for Disney Channel  Watch Full Episodes Movies  TV\"\n    },\n    {\n        \"question\": \"Where can I find the privacy policy for the 'Disney Channel  Watch Full Episodes Movies  TV' app?\",\n        \"answer\": \"http://disneyprivacycenter.com/\"\n    },\n    {\n        \"question\": \"Which one costs less? The 'KQ MiniSynth' app or the 'Sago Mini Superhero' app?\",\n        \"answer\": \"The 'KQ MiniSynth' app costs $5.99, the 'Sago Mini Superhero' app costs $2.99. So 'Sago Mini Superhero' is cheaper\"\n    }\n]\n</pre> examples = [     {         \"question\": \"Do the 'Sago Mini Superhero' and 'Disney Channel  Watch Full Episodes Movies  TV' apps require internet connection?\",         \"answer\": \"You can play Sago Mini Superhero without wi-fi or internet. Internet is required for Disney Channel  Watch Full Episodes Movies  TV\"     },     {         \"question\": \"Where can I find the privacy policy for the 'Disney Channel  Watch Full Episodes Movies  TV' app?\",         \"answer\": \"http://disneyprivacycenter.com/\"     },     {         \"question\": \"Which one costs less? The 'KQ MiniSynth' app or the 'Sago Mini Superhero' app?\",         \"answer\": \"The 'KQ MiniSynth' app costs $5.99, the 'Sago Mini Superhero' app costs $2.99. So 'Sago Mini Superhero' is cheaper\"     } ] <p>And now we upload them to Langsmith as a dataset if they haven't already been uploaded</p> In\u00a0[7]: Copied! <pre>dataset_name = \"AppStore Q&amp;A\"\n\n\ndef make_dataset():\n    \"\"\"Make and fill dataset if it doesnt exist\"\"\"\n    if client.has_dataset(dataset_name=dataset_name):\n        return client.read_dataset(dataset_name=dataset_name)\n\n    dataset = client.create_dataset(\n        dataset_name=dataset_name, description=\"App Store Data questions and answers\")\n\n    for example in examples:\n        client.create_example(\n            inputs={\"question\": example[\"question\"]}, outputs={\"answer\": example[\"answer\"]}, dataset_name=dataset.name)\n        \n    return dataset\n\ndataset = make_dataset()\n</pre> dataset_name = \"AppStore Q&amp;A\"   def make_dataset():     \"\"\"Make and fill dataset if it doesnt exist\"\"\"     if client.has_dataset(dataset_name=dataset_name):         return client.read_dataset(dataset_name=dataset_name)      dataset = client.create_dataset(         dataset_name=dataset_name, description=\"App Store Data questions and answers\")      for example in examples:         client.create_example(             inputs={\"question\": example[\"question\"]}, outputs={\"answer\": example[\"answer\"]}, dataset_name=dataset.name)              return dataset  dataset = make_dataset() <p>Our experiment is designed to test Gemini 1.5 Flash's performance across increasing context lengths. To execute this, we need several components:</p> <ol> <li>The App Store dataset</li> <li>Our evaluation dataset (Q&amp;As)</li> <li>An evaluation function</li> <li>A prediction function</li> </ol> In\u00a0[8]: Copied! <pre>from langchain_openai import ChatOpenAI\nfrom langchain.pydantic_v1 import BaseModel, Field\nfrom langchain.schema import SystemMessage, HumanMessage\nfrom langsmith.schemas import Run, Example\n\nclass EvaluationSchema(BaseModel):\n    \"\"\"An evaluation schema for assessing the correctness of an answer\"\"\"\n    reasoning: str = Field(\n        description=\"Detailed reasoning for the evaluation score\")\n    correct: bool = Field(\n        description=\"Whether the user's answer is correct or not\")\n\ndef qa_eval(root_run: Run, example: Example):\n    \"\"\"Evaluate the correctness of an answer to a given question\"\"\"\n    question = example.inputs[\"question\"]\n    user_answer = root_run.outputs[\"output\"]\n    correct_answer = example.outputs[\"answer\"]\n\n    if not question or not user_answer or not correct_answer:\n        return {\n            \"score\": 0,\n            \"key\": \"correctness\",\n            \"comment\": \"Question, user's answer, or correct answer is missing\"\n        }\n\n    llm = ChatOpenAI(\n        model=\"gpt-4o\", temperature=0.4).with_structured_output(EvaluationSchema)\n\n    system_prompt = f\"\"\"You are a judge tasked with evaluating a user's answer to a given question. \nYou will be provided with the question, the correct answer, and the user's thought process and answer.\n\nQuestion:\n{question}\n\nCorrect Answer:\n{correct_answer}\n\nYour job is to assess the user's answer and provide:\n1. Detailed reasoning for your evaluation, comparing the user's answer to the correct answer\n2. A boolean judgment on whether the user's answer is correct or not\n\nBe thorough in your reasoning and accurate in your judgment. Consider partial correctness and any nuances in the answers.\"\"\"\n\n    evaluation: EvaluationSchema = llm.invoke(\n        [SystemMessage(content=system_prompt),\n         HumanMessage(content=user_answer)]\n    )\n\n    score = 1 if evaluation.correct else 0\n\n    return {\n        \"score\": score,\n        \"key\": \"correctness\",\n        \"comment\": evaluation.reasoning\n    }\n</pre>  from langchain_openai import ChatOpenAI from langchain.pydantic_v1 import BaseModel, Field from langchain.schema import SystemMessage, HumanMessage from langsmith.schemas import Run, Example  class EvaluationSchema(BaseModel):     \"\"\"An evaluation schema for assessing the correctness of an answer\"\"\"     reasoning: str = Field(         description=\"Detailed reasoning for the evaluation score\")     correct: bool = Field(         description=\"Whether the user's answer is correct or not\")  def qa_eval(root_run: Run, example: Example):     \"\"\"Evaluate the correctness of an answer to a given question\"\"\"     question = example.inputs[\"question\"]     user_answer = root_run.outputs[\"output\"]     correct_answer = example.outputs[\"answer\"]      if not question or not user_answer or not correct_answer:         return {             \"score\": 0,             \"key\": \"correctness\",             \"comment\": \"Question, user's answer, or correct answer is missing\"         }      llm = ChatOpenAI(         model=\"gpt-4o\", temperature=0.4).with_structured_output(EvaluationSchema)      system_prompt = f\"\"\"You are a judge tasked with evaluating a user's answer to a given question.  You will be provided with the question, the correct answer, and the user's thought process and answer.  Question: {question}  Correct Answer: {correct_answer}  Your job is to assess the user's answer and provide: 1. Detailed reasoning for your evaluation, comparing the user's answer to the correct answer 2. A boolean judgment on whether the user's answer is correct or not  Be thorough in your reasoning and accurate in your judgment. Consider partial correctness and any nuances in the answers.\"\"\"      evaluation: EvaluationSchema = llm.invoke(         [SystemMessage(content=system_prompt),          HumanMessage(content=user_answer)]     )      score = 1 if evaluation.correct else 0      return {         \"score\": score,         \"key\": \"correctness\",         \"comment\": evaluation.reasoning     }  <p>This evaluation function allows us to assess Gemini 1.5 Flash's performance consistently across different context lengths and questions. By using a language model (GPT-4) as the judge, we can capture subtle aspects of correctness that might be missed by simpler, rule-based evaluation methods.</p> In\u00a0[9]: Copied! <pre>from tiktoken import get_encoding\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Gemini's 1M token limit\nmax_context_limit = 1000000\n\n\n# Util Functions\n\ndef count_tokens(text: str):\n    \"\"\"Count the number of tokens in a string\"\"\"\n    encoder = get_encoding(\"cl100k_base\")\n    return len(encoder.encode(text))\n\n\ndef row_to_string(row):\n    \"\"\"Convert a row to a string\"\"\"\n    app_string = f\"\"\"App Name: {row.name}\nSize: {round(row.size, 2)} MB\nPrice: {row.price} {row.currency}\nRating Count: {row.rating_count_tot}\nUser Rating: {row.user_rating}\nVersion: {row.ver}\nGenre: {row.prime_genre}\nDescription: {row.app_desc}\"\"\"\n    return app_string\n\n\ndef get_context(tokens: int):\n    \"\"\"Get the context for a given number of tokens\"\"\"\n    # Combine the golden df and the new_df\n    combined_df = pd.concat([golden_df, new_df])\n    app_strs: list[str] = []\n    delimiter = \"\\n================\\n\"\n\n    for i, row in enumerate(combined_df.itertuples()):\n        row_str = row_to_string(row)\n        num_tokens = count_tokens(\n            f\"{delimiter.join(app_strs)}{delimiter}{row_str}\")\n        if num_tokens &lt; tokens:  # If we havent hit the token limit, add the row\n            app_strs.append(row_str)\n        else:\n            break\n\n    # Randomize app strings\n    random.shuffle(app_strs)\n    return delimiter.join(app_strs)\n\n\ndef visualize_test_results(experiments):\n    \"\"\"Display a graph of the test results\"\"\"\n    # Step 1: Extract and process data\n    all_results = []\n    for exp in experiments:\n        df = client.get_test_results(project_name=exp[\"results\"].experiment_name)\n        df['tokens'] = exp['tokens']\n        all_results.append(df)\n    \n    # Combine all results into a single dataframe\n    combined_df = pd.concat(all_results, ignore_index=True)\n    \n    # Step 2: Sort by token count\n    combined_df = combined_df.sort_values('tokens')\n    \n    # Get unique questions\n    questions = combined_df['input.inputs.question'].unique()\n    \n    # Create a color palette\n    color_palette = sns.color_palette(\"husl\", n_colors=len(questions))\n    \n    # Step 3: Create the line graph\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    for i, question in enumerate(questions):\n        question_data = combined_df[combined_df['input.inputs.question'] == question]\n        ax.plot(question_data['tokens'], question_data['feedback.correctness'], \n                label=f'Question {i+1}', color=color_palette[i])\n    \n    ax.set_title('Test Results by Token Count')\n    ax.set_xlabel('Number of Tokens')\n    ax.set_ylabel('Correctness Score')\n    ax.legend(title='Questions', loc='center left', bbox_to_anchor=(1, 0.5))\n    \n    # Add questions as text below the graph\n    fig.text(0.1, 0.02, \"Questions:\", fontweight='bold')\n    for i, question in enumerate(questions):\n        fig.text(0.1, -0.02 - 0.03*i, f\"{i+1}. {question}\", fontsize=8, wrap=True)\n    \n    plt.tight_layout()\n    plt.subplots_adjust(bottom=0.3)  # Adjust this value to fit all questions\n    \n    return plt\n</pre> from tiktoken import get_encoding import random import matplotlib.pyplot as plt import seaborn as sns  # Gemini's 1M token limit max_context_limit = 1000000   # Util Functions  def count_tokens(text: str):     \"\"\"Count the number of tokens in a string\"\"\"     encoder = get_encoding(\"cl100k_base\")     return len(encoder.encode(text))   def row_to_string(row):     \"\"\"Convert a row to a string\"\"\"     app_string = f\"\"\"App Name: {row.name} Size: {round(row.size, 2)} MB Price: {row.price} {row.currency} Rating Count: {row.rating_count_tot} User Rating: {row.user_rating} Version: {row.ver} Genre: {row.prime_genre} Description: {row.app_desc}\"\"\"     return app_string   def get_context(tokens: int):     \"\"\"Get the context for a given number of tokens\"\"\"     # Combine the golden df and the new_df     combined_df = pd.concat([golden_df, new_df])     app_strs: list[str] = []     delimiter = \"\\n================\\n\"      for i, row in enumerate(combined_df.itertuples()):         row_str = row_to_string(row)         num_tokens = count_tokens(             f\"{delimiter.join(app_strs)}{delimiter}{row_str}\")         if num_tokens &lt; tokens:  # If we havent hit the token limit, add the row             app_strs.append(row_str)         else:             break      # Randomize app strings     random.shuffle(app_strs)     return delimiter.join(app_strs)   def visualize_test_results(experiments):     \"\"\"Display a graph of the test results\"\"\"     # Step 1: Extract and process data     all_results = []     for exp in experiments:         df = client.get_test_results(project_name=exp[\"results\"].experiment_name)         df['tokens'] = exp['tokens']         all_results.append(df)          # Combine all results into a single dataframe     combined_df = pd.concat(all_results, ignore_index=True)          # Step 2: Sort by token count     combined_df = combined_df.sort_values('tokens')          # Get unique questions     questions = combined_df['input.inputs.question'].unique()          # Create a color palette     color_palette = sns.color_palette(\"husl\", n_colors=len(questions))          # Step 3: Create the line graph     fig, ax = plt.subplots(figsize=(12, 8))          for i, question in enumerate(questions):         question_data = combined_df[combined_df['input.inputs.question'] == question]         ax.plot(question_data['tokens'], question_data['feedback.correctness'],                  label=f'Question {i+1}', color=color_palette[i])          ax.set_title('Test Results by Token Count')     ax.set_xlabel('Number of Tokens')     ax.set_ylabel('Correctness Score')     ax.legend(title='Questions', loc='center left', bbox_to_anchor=(1, 0.5))          # Add questions as text below the graph     fig.text(0.1, 0.02, \"Questions:\", fontweight='bold')     for i, question in enumerate(questions):         fig.text(0.1, -0.02 - 0.03*i, f\"{i+1}. {question}\", fontsize=8, wrap=True)          plt.tight_layout()     plt.subplots_adjust(bottom=0.3)  # Adjust this value to fit all questions          return plt In\u00a0[\u00a0]: Copied! <pre>from langchain_core.messages import SystemMessage, HumanMessage\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\n\nfrom langsmith.evaluation import evaluate\n\n\nclass Predictor:\n    def __init__(self, step=1, total_steps=20, model=\"gemini-1.5-flash\"):\n        self.step = step\n        self.total_steps = total_steps\n        self.model = model\n        self.llm = ChatGoogleGenerativeAI(model=model)\n        self.experiments = []\n\n    def predict(self, inputs: dict):\n        \"\"\"Prediction function for Gemini Experiment\"\"\"\n        tokens = (max_context_limit / self.total_steps) * self.step\n        context = get_context(tokens)\n\n        system_prompt = f\"\"\"You are tasked with answering user questions based on the the App Store data inside &lt;APP STORE DATA&gt;.\n&lt;APP STORE DATA&gt; contains a ton of public data about apps on the App Store. It is the most current and accurate source \\\nso be sure to ONLY answer based on the context in &lt;APP STORE DATA&gt;. You will be graded on accuracy so be very careful and \\\nmake sure you are as accurate as possible. First, think through your reasoning to answering the question before ultimately repeating \\\nthe question and giving your answer.\n\n&lt;APP STORE DATA&gt;\n{context}\n&lt;/APP STORE DATA&gt;\"\"\"\n        response = self.llm.invoke(\n            [SystemMessage(content=system_prompt), HumanMessage(content=inputs[\"question\"])])\n        return {\"output\": response.content}\n\n    def _run_eval(self):\n        \"\"\"Run a single evaluation for Gemini Experiment\"\"\"\n        tokens = (max_context_limit / self.total_steps) * self.step\n\n        result = evaluate(\n            self.predict,\n            data=client.list_examples(dataset_name=dataset_name),\n            evaluators=[qa_eval],\n            experiment_prefix=f\"{self.model}-{tokens}\"\n        )\n\n        # Append the results to the experiments list\n        self.experiments.append({\n            \"tokens\": tokens,\n            \"step\": self.step,\n            \"results\": result\n        })\n\n    def run(self):\n        \"\"\"Run a single step of the Gemini Experiment\"\"\"\n        print(f\"Running step {self.step} of the Gemini Experiment\")\n        self._run_eval()\n        # Increment the step\n        self.step += 1\n        # If we have more than 1 experiment, display the results\n        if len(self.experiments) &gt; 1:\n            visualize_test_results(self.experiments)\n\n    def run_all(self, reset=False, stop_at=None):\n        \"\"\"Run all steps of the Gemini Experiment\n\n        Args:\n            reset (bool, optional): Whether to reset the step counter. Defaults to False.\n            stop_at (int, optional): The step to stop at. Defaults to Predictor.total_steps.\n        \"\"\"\n        if stop_at is None:\n            stop_at = self.total_steps\n\n        if reset:\n            self.step = 1\n\n        while self.step &lt;= stop_at:\n            self.run()\n\n\neval = Predictor()\n</pre> from langchain_core.messages import SystemMessage, HumanMessage from langchain_google_genai import ChatGoogleGenerativeAI   from langsmith.evaluation import evaluate   class Predictor:     def __init__(self, step=1, total_steps=20, model=\"gemini-1.5-flash\"):         self.step = step         self.total_steps = total_steps         self.model = model         self.llm = ChatGoogleGenerativeAI(model=model)         self.experiments = []      def predict(self, inputs: dict):         \"\"\"Prediction function for Gemini Experiment\"\"\"         tokens = (max_context_limit / self.total_steps) * self.step         context = get_context(tokens)          system_prompt = f\"\"\"You are tasked with answering user questions based on the the App Store data inside .  contains a ton of public data about apps on the App Store. It is the most current and accurate source \\ so be sure to ONLY answer based on the context in . You will be graded on accuracy so be very careful and \\ make sure you are as accurate as possible. First, think through your reasoning to answering the question before ultimately repeating \\ the question and giving your answer.   {context} \"\"\"         response = self.llm.invoke(             [SystemMessage(content=system_prompt), HumanMessage(content=inputs[\"question\"])])         return {\"output\": response.content}      def _run_eval(self):         \"\"\"Run a single evaluation for Gemini Experiment\"\"\"         tokens = (max_context_limit / self.total_steps) * self.step          result = evaluate(             self.predict,             data=client.list_examples(dataset_name=dataset_name),             evaluators=[qa_eval],             experiment_prefix=f\"{self.model}-{tokens}\"         )          # Append the results to the experiments list         self.experiments.append({             \"tokens\": tokens,             \"step\": self.step,             \"results\": result         })      def run(self):         \"\"\"Run a single step of the Gemini Experiment\"\"\"         print(f\"Running step {self.step} of the Gemini Experiment\")         self._run_eval()         # Increment the step         self.step += 1         # If we have more than 1 experiment, display the results         if len(self.experiments) &gt; 1:             visualize_test_results(self.experiments)      def run_all(self, reset=False, stop_at=None):         \"\"\"Run all steps of the Gemini Experiment          Args:             reset (bool, optional): Whether to reset the step counter. Defaults to False.             stop_at (int, optional): The step to stop at. Defaults to Predictor.total_steps.         \"\"\"         if stop_at is None:             stop_at = self.total_steps          if reset:             self.step = 1          while self.step &lt;= stop_at:             self.run()   eval = Predictor() <p>Key aspects of this implementation:</p> <ol> <li><p>Context Generation: The <code>get_context</code> function generates a context of the appropriate length for each step of the experiment.</p> </li> <li><p>Incremental Context: The <code>step</code> and <code>total_steps</code> parameters allow us to incrementally increase the context length from 50,000 to 1,000,000 tokens. We have to use a class because our prediction function should only take the <code>inputs</code> dict.</p> </li> <li><p>System Prompt: We wrote a system prompt to instruct Gemini 1.5 Flash on how to approach the task. This prompt emphasizes:</p> <ul> <li>Using only the provided context</li> <li>The importance of accuracy</li> <li>The need for reasoning before answering</li> </ul> </li> <li><p>Model Invocation: We use the <code>ChatGoogleGenerativeAI</code> class from the <code>langchain_google_genai</code> library to interact with Gemini 1.5 Flash.</p> </li> </ol> <p>This prediction function allows us to systematically test Gemini 1.5 Flash's performance across varying context lengths while maintaining consistent instructions and evaluation criteria. By incrementing the <code>step</code> parameter, we can observe how the model's performance changes as it has access to more context.</p> In\u00a0[\u00a0]: Copied! <pre>eval.run()\n</pre> eval.run()"},{"location":"experiments/Context_Length_AppStoreV2/#context-length-experiment","title":"Context Length Experiment\u00b6","text":"<p>An experiment to test Gemini 1.5 Flash's ability to answer questions about up to 1 million tokens of context.</p> <p>Gemini 1.5 Flash, with its million-token context capability, represents a significant advancement in LLMs. This expanded context length opens up new possibilities for processing and understanding vast amounts of information. However, it also raises important questions about how effectively such models can utilize this extensive context in practical applications. How can we really know what to expect from an analysis of a 1 million token context?</p> <p>Traditionally, long-context models have been evaluated using \"needle in a haystack\" tests, where specific and usually irrelevant information is hidden within a large context to assess the model's retrieval capabilities. While valuable, these tests don't fully explore a model's ability to reason across and synthesize information from its entire context - a crucial skill for many real-world applications. They also lean on irrelevant needles, which I believe gives the LLM a crutch of parsing out anomalies in the text rather than understanding it.</p> <p>This study aims to address this gap by conducting a comprehensive evaluation of Gemini 1.5 Flash's question-answering capabilities across varying context lengths. Using a dataset derived from the Apple App Store, we design an experiment that systematically increases the context from 50,000 tokens to the full million-token capacity.</p> <p>Our primary objectives are to:</p> <ol> <li>Assess Gemini 1.5 Flash's performance in answering specific questions as the context length increases.</li> <li>Explore the practical implications of using such large context lengths in real-world scenarios.</li> </ol> <p>The experiment involves a set of questions about a curated set of apps, requiring the model to synthesize information from different parts of the context. By incrementally increasing the context length, we aim to understand not just the model's information retrieval capabilities, but its ability to reason across vast amounts of data.</p>"},{"location":"experiments/Context_Length_AppStoreV2/#setup","title":"Setup\u00b6","text":""},{"location":"experiments/Context_Length_AppStoreV2/#data-collection-and-preparation","title":"Data Collection and Preparation\u00b6","text":""},{"location":"experiments/Context_Length_AppStoreV2/#data-cleaning-and-preprocessing","title":"Data Cleaning and Preprocessing\u00b6","text":"<p>To prepare the data for our experiment, we perform several cleaning and preprocessing steps:</p> <ol> <li>Select relevant columns and rename for clarity</li> <li>Convert app size from bytes to megabytes</li> <li>Remove special characters from app names</li> <li>Filter out rows with empty names or zero ratings</li> <li>Remove entries with non-Latin characters in the description</li> </ol> <p>Here's the code implementing these steps:</p>"},{"location":"experiments/Context_Length_AppStoreV2/#golden-dataset-selection","title":"Golden Dataset Selection\u00b6","text":"<p>To create a controlled subset for our questions, we select a \"golden dataset\" of five apps. Rather than random selection, which could introduce variability across experiments, we chose a deterministic approach:</p>"},{"location":"experiments/Context_Length_AppStoreV2/#evaluation-dataset","title":"Evaluation Dataset\u00b6","text":"<p>Our evaluation dataset consists of three carefully crafted questions designed to test Gemini 1.5 Flash's ability to synthesize information across the App Store data context:</p>"},{"location":"experiments/Context_Length_AppStoreV2/#experiment","title":"Experiment\u00b6","text":""},{"location":"experiments/Context_Length_AppStoreV2/#evaluation-function","title":"Evaluation Function\u00b6","text":"<p>The evaluation function is a critical component of our experiment. It's responsible for:</p> <ol> <li>Comparing the model's output to the correct answer</li> <li>Providing a detailed assessment of the answer's correctness</li> <li>Assigning a binary score (correct or incorrect)</li> </ol> <p>Key aspects of this implementation:</p> <ol> <li><p>GPT-4o as Judge: We use GPT-4o (via the <code>ChatOpenAI</code> class) to evaluate the answers. This allows for nuanced understanding and assessment of the responses. If you haven't worked with LLM's as a judge, it may sound unreliable to have an LLM grade an LLM but it is very reliable in scenarios like this. You can think about it like this: GPT-4o is more than capable of validating that an answer is the same as the correct answer we give it. This is a much more simple task than the one we are giving Gemini Flash of actually generating the correct answer.</p> </li> <li><p>Structured Output: Using <code>with_structured_output(EvaluationSchema)</code> ensures that our evaluation consistently provides both reasoning and a binary correctness judgment that we can use programmatically.</p> </li> <li><p>Detailed Evaluation: The system prompt instructs the GPT-4o model to provide thorough reasoning, considering partial correctness and nuances in the answers.</p> </li> <li><p>Binary Scoring: While the evaluation includes detailed reasoning, the final score is binary (0 or 1) for simplicity in aggregating results across multiple questions and context lengths.</p> </li> </ol>"},{"location":"experiments/Context_Length_AppStoreV2/#prediction-function","title":"Prediction Function\u00b6","text":"<p>The prediction function is a crucial component of our experiment. It's responsible for:</p> <ol> <li>Taking a question from our evaluation dataset</li> <li>Generating a context of appropriate length</li> <li>Querying Gemini 1.5 Flash with the question and context</li> <li>Returning the model's response</li> </ol> <p>Here's our implementation:</p>"},{"location":"experiments/Context_Length_AppStoreV2/#lets-run-our-experiment","title":"Lets run our experiment!\u00b6","text":""},{"location":"experiments/Context_Length_AppStoreV2/#results","title":"Results\u00b6","text":"<p>The results of our experiment with Gemini 1.5 Flash are remarkable in their consistency. Across all context lengths, from 50,000 tokens all the way up to the full million-token capacity, Gemini 1.5 Flash achieved 100% accuracy in answering our test questions!</p> <p>View Test Results on LangSmith</p> question 50k 100k 150k 200k 250k 300k 350k 400k 450k 500k 550k 600k 950k Which one costs less? The 'KQ MiniSynth' app or the 'Sago Mini Superhero' app? 1 1 1 1 1 1 1 1 1 1 1 1 1 Where can I find the privacy policy for the 'Disney Channel  Watch Full Episodes Movies  TV' app? 1 1 1 1 1 1 1 1 1 1 1 1 1 Do the 'Sago Mini Superhero' and 'Disney Channel  Watch Full Episodes Movies  TV' apps require internet connection? 1 1 1 1 1 1 1 1 1 1 1 1 1 <p>Key Findings:</p> <ol> <li>Perfect Accuracy: Gemini 1.5 Flash maintained 100% correctness across all context lengths, from 50,000 to 1,000,000 tokens.</li> <li>Scalability: The model's performance did not degrade as the context length increased, demonstrating robust scalability.</li> <li>Consistency: Regardless of the amount of context provided, Gemini 1.5 Flash consistently provided accurate answers, indicating strong information synthesis capabilities.</li> </ol> <p>It's important to note that our experiment was carefully designed to avoid numerical reasoning and relational queries. Previous experiments have shown that Gemini Flash struggles with tasks involving logic around numbers, such as identifying \"the highest rated\" or \"Top 5 by size\" apps. Our questions focused on factual retrieval and simple comparisons, areas where the model excels.</p>"},{"location":"experiments/Context_Length_AppStoreV2/#implications","title":"Implications:\u00b6","text":"<ol> <li><p>Comprehensive Document Analysis: Organizations can now process entire documents or databases in a single query. For example, a company could input all its policy documents, employee handbooks, and project reports into Gemini 1.5 Flash. This would allow for quick and accurate answers to complex queries that span multiple documents, potentially saving hours of manual searching and cross-referencing.</p> </li> <li><p>Enhanced Customer Support: Customer service departments could leverage Gemini 1.5 Flash to create incredibly knowledgeable chatbots. By inputting all product information, past customer interactions, and frequently asked questions, these chatbots could provide accurate, context-aware responses to customer queries. This could significantly reduce response times and improve customer satisfaction while decreasing the workload on human customer service representatives.</p> </li> <li><p>Improved Contract Analysis: Legal departments and law firms could use Gemini 1.5 Flash to analyze lengthy contracts and legal documents. By inputting multiple related contracts, case law, and regulatory information, lawyers could quickly get accurate answers to specific legal questions, potentially speeding up contract review processes and reducing the risk of overlooking important clauses or legal precedents.</p> </li> </ol> <p>While these results are extremely promising, it's crucial to remember that they are based on a specific dataset and set of questions. The model's performance on numerical reasoning and relational queries remains a limitation. Further testing across diverse domains and more complex query types would be beneficial to fully understand the capabilities and limitations of Gemini 1.5 Flash in real-world scenarios.</p> <p>Nevertheless, these results mark a significant leap forward in the field of large language models, particularly in handling and analyzing vast amounts of textual information. The ability to maintain perfect accuracy across such a large context opens up exciting possibilities for businesses dealing with large volumes of documents and data.</p>"},{"location":"experiments/PII-Obfuscation/","title":"PII Obfuscation","text":"In\u00a0[\u00a0]: Copied! <pre># Install required libraries\n%pip install -qU llm-guard langgraph langchain-core langchain-openai python-dotenv\n</pre> # Install required libraries %pip install -qU llm-guard langgraph langchain-core langchain-openai python-dotenv  <p>Load environment variables (e.g., API keys) from a <code>.env</code> file.</p> In\u00a0[\u00a0]: Copied! <pre># Load environment variables from a .env file\nfrom dotenv import load_dotenv\nload_dotenv()\n</pre> # Load environment variables from a .env file from dotenv import load_dotenv load_dotenv()  In\u00a0[\u00a0]: Copied! <pre># Import Vault and Anonymize classes from llm-guard\nfrom llm_guard.vault import Vault\nfrom llm_guard.input_scanners import Anonymize\n\n# Initialize a vault to store PII and create a scanner for PII anonymization\nvault = Vault()\nscanner = Anonymize(vault)\n</pre> # Import Vault and Anonymize classes from llm-guard from llm_guard.vault import Vault from llm_guard.input_scanners import Anonymize  # Initialize a vault to store PII and create a scanner for PII anonymization vault = Vault() scanner = Anonymize(vault)  In\u00a0[\u00a0]: Copied! <pre># Example: Mask PII in a sample string\nresponse = scanner.scan(\"Hi my email is johnsmith@gmail.com\")\nprint(f\"Sanitized Prompt: {response[0]}\")\n</pre> # Example: Mask PII in a sample string response = scanner.scan(\"Hi my email is johnsmith@gmail.com\") print(f\"Sanitized Prompt: {response[0]}\")  In\u00a0[\u00a0]: Copied! <pre># The PII is stored in the vault\nscanner._vault.get()\n</pre> # The PII is stored in the vault scanner._vault.get()  In\u00a0[\u00a0]: Copied! <pre># Subsequent scans will add new PII to the vault\nresponse = scanner.scan(\"My name is John Smith\")\nscanner._vault.get()\n</pre> # Subsequent scans will add new PII to the vault response = scanner.scan(\"My name is John Smith\") scanner._vault.get()  In\u00a0[\u00a0]: Copied! <pre># Define a function to unmask text using the PII stored in the vault\ndef unmask(scanner: Anonymize, text: str):\n    # Retrieve the list of PII entities from the vault\n    entities = scanner._vault.get()\n\n    # Loop through the entities and replace the tokens with the original PII strings\n    for token, original_pii in entities:\n        text = text.replace(token, original_pii)\n\n    return text\n</pre> # Define a function to unmask text using the PII stored in the vault def unmask(scanner: Anonymize, text: str):     # Retrieve the list of PII entities from the vault     entities = scanner._vault.get()      # Loop through the entities and replace the tokens with the original PII strings     for token, original_pii in entities:         text = text.replace(token, original_pii)      return text  In\u00a0[\u00a0]: Copied! <pre>import json\nfrom langchain_core.tools import tool\n\n@tool\ndef account_lookup(masked_name: str, masked_email: str):\n    \"\"\"\n    Look up a user's account information based on their name and email.\n    Expects inputs to be masked PII tokens.\n\n    Parameters:\n    - masked_name (str): Masked token representing the user's name.\n    - masked_email (str): Masked token representing the user's email.\n\n    Returns:\n    - dict: Masked account information.\n    \"\"\"\n\n    # Unmask the arguments to get the real PII for account lookup\n    real_name = unmask(scanner, masked_name)\n    real_email = unmask(scanner, masked_email)\n\n    # Mock account lookup process\n    print(f\"Looking up account for {real_name} with email {real_email}\")\n\n    mock_account_data = {\n        \"name\": masked_name,\n        \"email\": masked_email,\n        \"username\": \"jsmith22\",\n        \"phone_number\": \"(555) 555-1234\",\n        \"address\": \"1234 Main St, Anytown, USA\",\n        \"account_balance\": \"$1,000.75\"\n    }\n\n    print(f\"Found account: {mock_account_data['username']}\")\n\n    # Mask any PII in the account data before returning\n    # Mask the dict by scanning its JSON string representation\n    masked_account_str = scanner.scan(json.dumps(mock_account_data))[0]\n    masked_account_data = json.loads(masked_account_str)\n\n    # Manually mask fields that may not be automatically masked\n    # For example, mask the username and address\n    scanner._vault.append((\"[REDACTED_USERNAME_1]\", mock_account_data[\"username\"]))\n    scanner._vault.append((\"[REDACTED_ADDRESS_1]\", mock_account_data[\"address\"]))\n    masked_account_data[\"username\"] = \"[REDACTED_USERNAME_1]\"\n    masked_account_data[\"address\"] = \"[REDACTED_ADDRESS_1]\"\n\n    # Return the masked account data to the LLM\n    return masked_account_data\n</pre> import json from langchain_core.tools import tool  @tool def account_lookup(masked_name: str, masked_email: str):     \"\"\"     Look up a user's account information based on their name and email.     Expects inputs to be masked PII tokens.      Parameters:     - masked_name (str): Masked token representing the user's name.     - masked_email (str): Masked token representing the user's email.      Returns:     - dict: Masked account information.     \"\"\"      # Unmask the arguments to get the real PII for account lookup     real_name = unmask(scanner, masked_name)     real_email = unmask(scanner, masked_email)      # Mock account lookup process     print(f\"Looking up account for {real_name} with email {real_email}\")      mock_account_data = {         \"name\": masked_name,         \"email\": masked_email,         \"username\": \"jsmith22\",         \"phone_number\": \"(555) 555-1234\",         \"address\": \"1234 Main St, Anytown, USA\",         \"account_balance\": \"$1,000.75\"     }      print(f\"Found account: {mock_account_data['username']}\")      # Mask any PII in the account data before returning     # Mask the dict by scanning its JSON string representation     masked_account_str = scanner.scan(json.dumps(mock_account_data))[0]     masked_account_data = json.loads(masked_account_str)      # Manually mask fields that may not be automatically masked     # For example, mask the username and address     scanner._vault.append((\"[REDACTED_USERNAME_1]\", mock_account_data[\"username\"]))     scanner._vault.append((\"[REDACTED_ADDRESS_1]\", mock_account_data[\"address\"]))     masked_account_data[\"username\"] = \"[REDACTED_USERNAME_1]\"     masked_account_data[\"address\"] = \"[REDACTED_ADDRESS_1]\"      # Return the masked account data to the LLM     return masked_account_data  In\u00a0[\u00a0]: Copied! <pre>from typing_extensions import TypedDict\n\nfrom langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the LLM with the account_lookup tool bound to it\nllm_with_tools = ChatOpenAI(\n    model=\"gpt-4o-mini\", temperature=0).bind_tools([account_lookup])\n\n# Define the system prompt for the assistant\nsystem_prompt = \"\"\"You are a customer assistant agent. Your job is to look up the user's account information if they request it.\nThe user's personal details are masked in the transcript and replaced with tokens (e.g., '[REDACTED_NAME_1]'). Use the tokens in the\naccount lookup tool arguments when invoking it.\"\"\"\n\n# Define the agent's state structure\nclass AgentState(TypedDict):\n    # We want to allow overwriting of messages so that we can mask and unmask them in pre and post processing\n    messages: list[BaseMessage]\n</pre> from typing_extensions import TypedDict  from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage from langchain_openai import ChatOpenAI  # Initialize the LLM with the account_lookup tool bound to it llm_with_tools = ChatOpenAI(     model=\"gpt-4o-mini\", temperature=0).bind_tools([account_lookup])  # Define the system prompt for the assistant system_prompt = \"\"\"You are a customer assistant agent. Your job is to look up the user's account information if they request it. The user's personal details are masked in the transcript and replaced with tokens (e.g., '[REDACTED_NAME_1]'). Use the tokens in the account lookup tool arguments when invoking it.\"\"\"  # Define the agent's state structure class AgentState(TypedDict):     # We want to allow overwriting of messages so that we can mask and unmask them in pre and post processing     messages: list[BaseMessage]  In\u00a0[\u00a0]: Copied! <pre>import copy\n\n# Preprocessing node: Mask PII in the messages before sending them to the LLM\ndef pre_process(state: AgentState) -&gt; AgentState:\n    \"\"\"\n    Mask PII in the messages before sending them to the LLM.\n    \"\"\"\n    # Deep copy the messages to avoid modifying the original state\n    messages = copy.deepcopy(state[\"messages\"])\n    for message in messages:\n        # Replace the message content with the masked version\n        message.content = scanner.scan(message.content)[0]\n    return {\"messages\": messages}\n\n# Postprocessing node: Unmask PII in the messages before returning them to the user\ndef post_process(state: AgentState) -&gt; AgentState:\n    \"\"\"\n    Unmask PII in the messages before returning them to the user.\n    \"\"\"\n    # Deep copy the messages to avoid modifying the original state\n    messages = copy.deepcopy(state[\"messages\"])\n    for message in messages:\n        # Replace the message content with the unmasked version\n        message.content = unmask(scanner, message.content)\n    return {\"messages\": messages}\n\n# Model call node: Invoke the LLM with the masked messages\ndef call_model(state: AgentState) -&gt; AgentState:\n    \"\"\"\n    Call the LLM with the masked messages.\n    \"\"\"\n    messages = state[\"messages\"]\n    # Add the system prompt to the beginning of the messages array\n    system_message = SystemMessage(content=system_prompt)\n    # Invoke the LLM\n    response = llm_with_tools.invoke([system_message] + messages)\n    # Return the messages including the LLM's response\n    return {\"messages\": messages + [response]}\n</pre> import copy  # Preprocessing node: Mask PII in the messages before sending them to the LLM def pre_process(state: AgentState) -&gt; AgentState:     \"\"\"     Mask PII in the messages before sending them to the LLM.     \"\"\"     # Deep copy the messages to avoid modifying the original state     messages = copy.deepcopy(state[\"messages\"])     for message in messages:         # Replace the message content with the masked version         message.content = scanner.scan(message.content)[0]     return {\"messages\": messages}  # Postprocessing node: Unmask PII in the messages before returning them to the user def post_process(state: AgentState) -&gt; AgentState:     \"\"\"     Unmask PII in the messages before returning them to the user.     \"\"\"     # Deep copy the messages to avoid modifying the original state     messages = copy.deepcopy(state[\"messages\"])     for message in messages:         # Replace the message content with the unmasked version         message.content = unmask(scanner, message.content)     return {\"messages\": messages}  # Model call node: Invoke the LLM with the masked messages def call_model(state: AgentState) -&gt; AgentState:     \"\"\"     Call the LLM with the masked messages.     \"\"\"     messages = state[\"messages\"]     # Add the system prompt to the beginning of the messages array     system_message = SystemMessage(content=system_prompt)     # Invoke the LLM     response = llm_with_tools.invoke([system_message] + messages)     # Return the messages including the LLM's response     return {\"messages\": messages + [response]}  In\u00a0[\u00a0]: Copied! <pre># Define the custom tool node\nclass ToolNode:\n    \"\"\"\n    A node that runs the tools requested in the last AIMessage.\n    \"\"\"\n\n    def __init__(self, tools: list) -&gt; None:\n        self.tools_by_name = {tool.name: tool for tool in tools}\n\n    def __call__(self, inputs: dict):\n        # Retrieve the messages from the inputs\n        if messages := inputs.get(\"messages\", []):\n            # Get the last message (from the AI)\n            message = messages[-1]\n        else:\n            raise ValueError(\"No message found in input\")\n\n        outputs = []\n        # Process each tool call in the message\n        for tool_call in message.tool_calls:\n            # Invoke the tool with the provided arguments\n            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n                tool_call[\"args\"]\n            )\n            # Create a ToolMessage with the result\n            outputs.append(\n                ToolMessage(\n                    content=json.dumps(tool_result),\n                    name=tool_call[\"name\"],\n                    tool_call_id=tool_call[\"id\"],\n                )\n            )\n        # Return the messages including the tool responses\n        return {\"messages\": messages + outputs}\n</pre> # Define the custom tool node class ToolNode:     \"\"\"     A node that runs the tools requested in the last AIMessage.     \"\"\"      def __init__(self, tools: list) -&gt; None:         self.tools_by_name = {tool.name: tool for tool in tools}      def __call__(self, inputs: dict):         # Retrieve the messages from the inputs         if messages := inputs.get(\"messages\", []):             # Get the last message (from the AI)             message = messages[-1]         else:             raise ValueError(\"No message found in input\")          outputs = []         # Process each tool call in the message         for tool_call in message.tool_calls:             # Invoke the tool with the provided arguments             tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(                 tool_call[\"args\"]             )             # Create a ToolMessage with the result             outputs.append(                 ToolMessage(                     content=json.dumps(tool_result),                     name=tool_call[\"name\"],                     tool_call_id=tool_call[\"id\"],                 )             )         # Return the messages including the tool responses         return {\"messages\": messages + outputs}  In\u00a0[\u00a0]: Copied! <pre>def route_llm_output(state: AgentState) -&gt; str:\n    \"\"\"\n    Determine the next node based on the LLM's output.\n\n    Returns:\n    - \"tool\" if the LLM invoked a tool.\n    - \"end\" if the LLM produced a content response.\n    \"\"\"\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tool\"\n    else:\n        return \"end\"\n</pre> def route_llm_output(state: AgentState) -&gt; str:     \"\"\"     Determine the next node based on the LLM's output.      Returns:     - \"tool\" if the LLM invoked a tool.     - \"end\" if the LLM produced a content response.     \"\"\"     messages = state[\"messages\"]     last_message = messages[-1]     if last_message.tool_calls:         return \"tool\"     else:         return \"end\"  In\u00a0[\u00a0]: Copied! <pre>from langgraph.graph import StateGraph\n\ngraph_builder = StateGraph(AgentState)\n\n# Add the custom nodes\ngraph_builder.add_node(\"Preprocess\", pre_process)\ngraph_builder.add_node(\"Call Model\", call_model)\ngraph_builder.add_node(\"Post Process\", post_process)\n\n# Add the custom tool node\ngraph_builder.add_node(\"Tool Call\", ToolNode(tools=[account_lookup]))\n\n# Define the edges\n# After preprocessing, call the model\ngraph_builder.add_edge(\"Preprocess\", \"Call Model\")\n\n# After the model call, route based on the LLM's output\ngraph_builder.add_conditional_edges(\n    \"Call Model\",\n    route_llm_output,\n    {\"tool\": \"Tool Call\", \"end\": \"Post Process\"}\n)\n\n# After tool calls, return to the model call node\ngraph_builder.add_edge(\"Tool Call\", \"Call Model\")\n\n# Set entry and finish points\ngraph_builder.set_entry_point(\"Preprocess\")\ngraph_builder.set_finish_point(\"Post Process\")\n</pre> from langgraph.graph import StateGraph  graph_builder = StateGraph(AgentState)  # Add the custom nodes graph_builder.add_node(\"Preprocess\", pre_process) graph_builder.add_node(\"Call Model\", call_model) graph_builder.add_node(\"Post Process\", post_process)  # Add the custom tool node graph_builder.add_node(\"Tool Call\", ToolNode(tools=[account_lookup]))  # Define the edges # After preprocessing, call the model graph_builder.add_edge(\"Preprocess\", \"Call Model\")  # After the model call, route based on the LLM's output graph_builder.add_conditional_edges(     \"Call Model\",     route_llm_output,     {\"tool\": \"Tool Call\", \"end\": \"Post Process\"} )  # After tool calls, return to the model call node graph_builder.add_edge(\"Tool Call\", \"Call Model\")  # Set entry and finish points graph_builder.set_entry_point(\"Preprocess\") graph_builder.set_finish_point(\"Post Process\")  In\u00a0[\u00a0]: Copied! <pre># Initialize a new vault and scanner for a fresh session\nvault = Vault()\nscanner = Anonymize(vault)\n\n# Compile the graph to create the agent\napp = graph_builder.compile()\n</pre> # Initialize a new vault and scanner for a fresh session vault = Vault() scanner = Anonymize(vault)  # Compile the graph to create the agent app = graph_builder.compile()  In\u00a0[\u00a0]: Copied! <pre># Simulate a user message containing PII\noutput: AgentState = app.invoke(\n    {\"messages\": [HumanMessage(content=\"Hi my email is johnsmith@gmail.com\")]})\n\n# Print the output state\nprint(output)\n</pre> # Simulate a user message containing PII output: AgentState = app.invoke(     {\"messages\": [HumanMessage(content=\"Hi my email is johnsmith@gmail.com\")]})  # Print the output state print(output)  In\u00a0[\u00a0]: Copied! <pre># Simulate a follow-up user message containing PII\noutput: AgentState = app.invoke(\n    {\"messages\": output[\"messages\"] + [HumanMessage(content=\"Yea my name is John Smith. What is my address and account balance?\")]})\n\n# Print the output state\nprint(output)\n</pre> # Simulate a follow-up user message containing PII output: AgentState = app.invoke(     {\"messages\": output[\"messages\"] + [HumanMessage(content=\"Yea my name is John Smith. What is my address and account balance?\")]})  # Print the output state print(output)  In\u00a0[\u00a0]: Copied! <pre># Print the conversation messages in a readable format\nfor message in output[\"messages\"]:\n    message.pretty_print()\n</pre> # Print the conversation messages in a readable format for message in output[\"messages\"]:     message.pretty_print()  <p>In the resulting trace, we can see that the PII was masked internally at each step before finally being unmasked for the output.</p> <p>View the trace</p>"},{"location":"experiments/PII-Obfuscation/#pii-obfuscation","title":"PII Obfuscation\u00b6","text":"<p>This notebook demonstrates how PII (Personally Identifiable Information) obfuscation can work within the context of an LLM Agent.</p>"},{"location":"experiments/PII-Obfuscation/#overview","title":"Overview\u00b6","text":"<p>The goal is to prevent the LLM from seeing any PII. We achieve this by following this flow:</p> <ol> <li>Mask the raw user message upon arrival.</li> <li>Pass the masked message (along with chat history) to the LLM.</li> <li>The LLM invokes a tool using PII tokens instead of actual PII.</li> <li>Inside the tool, access the vault to unmask the PII.</li> <li>Invoke the tool with real user data and get a response.</li> <li>Mask the response and return it to the LLM, allowing it to respond using tokens.</li> <li>Unmask the final response before displaying it to the user.</li> </ol>"},{"location":"experiments/PII-Obfuscation/#setup","title":"Setup\u00b6","text":"<p>First, install the required libraries.</p>"},{"location":"experiments/PII-Obfuscation/#initializing-pii-vault-and-scanner","title":"Initializing PII Vault and Scanner\u00b6","text":"<p>We use <code>llm-guard</code> to handle PII masking and vault management.</p>"},{"location":"experiments/PII-Obfuscation/#example-masking-pii","title":"Example: Masking PII\u00b6","text":""},{"location":"experiments/PII-Obfuscation/#adding-more-pii-to-the-vault","title":"Adding More PII to the Vault\u00b6","text":""},{"location":"experiments/PII-Obfuscation/#unmasking-function","title":"Unmasking Function\u00b6","text":"<p>We define a function to unmask text using the PII stored in the vault.</p>"},{"location":"experiments/PII-Obfuscation/#creating-the-account-lookup-tool","title":"Creating the Account Lookup Tool\u00b6","text":"<p>We create a mock account lookup function and wrap it with a <code>@tool</code> decorator for use in LangGraph.</p> <p>The tool:</p> <ol> <li>Unmasks the input arguments.</li> <li>Performs the account lookup (mocked).</li> <li>Masks the output.</li> <li>Returns the masked output.</li> </ol>"},{"location":"experiments/PII-Obfuscation/#building-the-agent-with-langgraph","title":"Building the Agent with LangGraph\u00b6","text":"<p>We use LangGraph to build the agent. The state only needs to track the <code>messages</code> since we access the vault via the <code>scanner</code> object.</p>"},{"location":"experiments/PII-Obfuscation/#defining-the-nodes","title":"Defining the Nodes\u00b6","text":"<p>We define the preprocessing, postprocessing, and model call nodes.</p>"},{"location":"experiments/PII-Obfuscation/#custom-tool-node","title":"Custom Tool Node\u00b6","text":"<p>We need to customize the tool node to return the full messages array because we overwrite messages returned by nodes. The prebuilt <code>ToolNode</code> only returns the tool messages.</p>"},{"location":"experiments/PII-Obfuscation/#conditional-routing","title":"Conditional Routing\u00b6","text":"<p>We need to route to the appropriate node based on whether the LLM's output is a tool invocation or a content response.</p>"},{"location":"experiments/PII-Obfuscation/#building-the-graph","title":"Building the Graph\u00b6","text":"<p>We assemble the nodes and define the edges to build the agent.</p>"},{"location":"experiments/PII-Obfuscation/#testing-the-agent","title":"Testing the Agent\u00b6","text":"<p>We instantiate the agent and test it with sample messages containing PII.</p>"},{"location":"experiments/PII-Obfuscation/#first-user-message","title":"First User Message\u00b6","text":""},{"location":"experiments/PII-Obfuscation/#second-user-message","title":"Second User Message\u00b6","text":""},{"location":"experiments/PII-Obfuscation/#displaying-the-conversation","title":"Displaying the Conversation\u00b6","text":""},{"location":"experiments/Prompt_Caching/","title":"Prompt Caching","text":"In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>%pip install -qU google-generativeai python-dotenv pandas anthropic\n</pre> %pip install -qU google-generativeai python-dotenv pandas anthropic  In\u00a0[\u00a0]: Copied! <pre>import os\nimport google.generativeai as genai\nfrom google.generativeai import caching\nimport datetime\nimport time\nfrom dotenv import load_dotenv\nfrom utils import get_app_store_data, get_context\n\nload_dotenv()\n\ngenai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n</pre> import os import google.generativeai as genai from google.generativeai import caching import datetime import time from dotenv import load_dotenv from utils import get_app_store_data, get_context  load_dotenv()  genai.configure(api_key=os.environ['GOOGLE_API_KEY'])   In\u00a0[\u00a0]: Copied! <pre># Get app store data\ndf = get_app_store_data()\n\n# Get 150,000 tokens of context\napp_data_str, app_df = get_context(150000, df)\n</pre> # Get app store data df = get_app_store_data()  # Get 150,000 tokens of context app_data_str, app_df = get_context(150000, df)  In\u00a0[\u00a0]: Copied! <pre>system_prompt = \"\"\"You are an App Store Data Analyzer. You should analyze the provided app store data and answer the user's questions.\n\n- Only use the App Store Data provided in your context.\n- Do not answer questions you are not confident in answering because the answer can't be found in the provided context.\n- Think through your answer slowly, step by step before providing the final answer.\"\"\"\n</pre> system_prompt = \"\"\"You are an App Store Data Analyzer. You should analyze the provided app store data and answer the user's questions.  - Only use the App Store Data provided in your context. - Do not answer questions you are not confident in answering because the answer can't be found in the provided context. - Think through your answer slowly, step by step before providing the final answer.\"\"\"  In\u00a0[\u00a0]: Copied! <pre>print(app_data_str)\n</pre> print(app_data_str) In\u00a0[\u00a0]: Copied! <pre>examples = [\n    \"What app has the most ratings?\",\n    \"What are the features for the app 'Online Head Ball'?\",\n    \"What is the most expensive app in the 'Games' category?\",\n    # \"Which app has the longest description in the app store?\",\n    # \"What is the average rating of all free apps?\",\n    # \"Identify any app that is paid and has fewer than 100 ratings.\",\n    # \"List all apps that are categorized under 'Games' and have more than 400,000 ratings.\",\n    # \"Which app has the lowest price in the app store?\",\n    # \"Find all apps that have a title starting with the letter 'A'.\",\n    # \"What is the total number of ratings for all apps in the 'Health &amp; Fitness' category?\"\n]\n</pre> examples = [     \"What app has the most ratings?\",     \"What are the features for the app 'Online Head Ball'?\",     \"What is the most expensive app in the 'Games' category?\",     # \"Which app has the longest description in the app store?\",     # \"What is the average rating of all free apps?\",     # \"Identify any app that is paid and has fewer than 100 ratings.\",     # \"List all apps that are categorized under 'Games' and have more than 400,000 ratings.\",     # \"Which app has the lowest price in the app store?\",     # \"Find all apps that have a title starting with the letter 'A'.\",     # \"What is the total number of ratings for all apps in the 'Health &amp; Fitness' category?\" ] <p>Gemini says</p> <p>The model doesn't make any distinction between cached tokens and regular input tokens. Cached content is simply a prefix to the prompt.</p> <p>That means we are pretty much only able to cache the system prompt. So you wouldnt be able to cache search results or function call responses. We cant cache tools. It does make sense though, in an agent the only thing that is static is the system prompt.</p> In\u00a0[\u00a0]: Copied! <pre># Create a cache with a 5 minute TTL\ncache = caching.CachedContent.create(\n    model='models/gemini-1.5-flash-001',\n    display_name='App Store Data',  # used to identify the cache\n    system_instruction=system_prompt,\n    contents=[app_data_str],\n    ttl=datetime.timedelta(minutes=1),\n)\n</pre> # Create a cache with a 5 minute TTL cache = caching.CachedContent.create(     model='models/gemini-1.5-flash-001',     display_name='App Store Data',  # used to identify the cache     system_instruction=system_prompt,     contents=[app_data_str],     ttl=datetime.timedelta(minutes=1), ) In\u00a0[\u00a0]: Copied! <pre>model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n\ndef chat_gemini(query: str):\n    # Call the model\n    response = model.generate_content([(query)])\n\n    print(response.usage_metadata)\n    print(f\"Question: {example}\")\n    print(f\"Answer: {response.text}\")\n    print(\"\\n======\\n\")\n    return response\n\nfor example in examples:\n    chat_gemini(example)\n</pre> model = genai.GenerativeModel.from_cached_content(cached_content=cache)  def chat_gemini(query: str):     # Call the model     response = model.generate_content([(query)])      print(response.usage_metadata)     print(f\"Question: {example}\")     print(f\"Answer: {response.text}\")     print(\"\\n======\\n\")     return response  for example in examples:     chat_gemini(example) In\u00a0[\u00a0]: Copied! <pre>import anthropic\n\nclient = anthropic.Anthropic()\n</pre> import anthropic  client = anthropic.Anthropic() In\u00a0[\u00a0]: Copied! <pre># Literal of models\nfrom typing import Literal\n\nmodels = Literal[\"claude-3-5-sonnet-20240620\", \"claude-3-haiku-20240307\", \"claude-3-opus-20240229\"]\n\ndef chat_anthropic(query: str, model: models = \"claude-3-5-sonnet-20240620\"):\n    response = client.beta.prompt_caching.messages.create(\n        model=model,\n        max_tokens=1024,\n        system=[\n            {\n                \"type\": \"text\",\n                \"text\": system_prompt,\n            },\n            {\n                \"type\": \"text\",\n                \"text\": app_data_str,\n                \"cache_control\": {\"type\": \"ephemeral\"}\n            }\n        ],\n        messages=[{\"role\": \"user\", \"content\": query}],\n    )\n\n    print(response)\n    print(f\"Question: {query}\")\n    print(f\"Answer: {response}\")\n    print(\"\\n======\\n\")\n    return response\n</pre> # Literal of models from typing import Literal  models = Literal[\"claude-3-5-sonnet-20240620\", \"claude-3-haiku-20240307\", \"claude-3-opus-20240229\"]  def chat_anthropic(query: str, model: models = \"claude-3-5-sonnet-20240620\"):     response = client.beta.prompt_caching.messages.create(         model=model,         max_tokens=1024,         system=[             {                 \"type\": \"text\",                 \"text\": system_prompt,             },             {                 \"type\": \"text\",                 \"text\": app_data_str,                 \"cache_control\": {\"type\": \"ephemeral\"}             }         ],         messages=[{\"role\": \"user\", \"content\": query}],     )      print(response)     print(f\"Question: {query}\")     print(f\"Answer: {response}\")     print(\"\\n======\\n\")     return response In\u00a0[\u00a0]: Copied! <pre>for example in examples:\n    chat_anthropic(example)\n</pre> for example in examples:     chat_anthropic(example) <p>Wow, so the rate limits are insanely low. We get 40k tokens per minute with 1M tokens PER DAY. We really cant do anything with that so I think thats where we end things. It would be interesting to try it but it doesnt look like this would even be an option for our experiment without getting a custom account.</p> <p>The max tier gives us 50,000,000 tokens per day with 400k tokens per minute. Thats enough to do 2 experiments per minute.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"experiments/Prompt_Caching/#prompt-caching","title":"Prompt Caching\u00b6","text":"<p>Gonna load all of the app data in and ask it questions with follow ups</p> <p>https://ai.google.dev/gemini-api/docs/caching?lang=python</p>"},{"location":"experiments/Prompt_Caching/#setup","title":"Setup\u00b6","text":""},{"location":"experiments/Prompt_Caching/#load-env","title":"Load Env\u00b6","text":""},{"location":"experiments/Prompt_Caching/#get-all-of-the-app-store-data","title":"Get all of the app store data\u00b6","text":""},{"location":"experiments/Prompt_Caching/#system-prompt-and-examples","title":"System Prompt and Examples\u00b6","text":""},{"location":"experiments/Prompt_Caching/#gemini","title":"Gemini\u00b6","text":""},{"location":"experiments/Prompt_Caching/#anthropic","title":"Anthropic\u00b6","text":"<p>https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching</p> <p>Cache tools https://python.langchain.com/v0.2/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html#langchain_anthropic.chat_models.ChatAnthropic.bind_tools</p> <p>Normal Chat https://docs.anthropic.com/en/docs/quickstart#call-the-api</p>"},{"location":"experiments/utils/","title":"Utils","text":"In\u00a0[\u00a0]: Copied! <pre>import os\n</pre> import os In\u00a0[\u00a0]: Copied! <pre>def get_app_store_data():\n    try:\n        import pandas as pd\n        return pd.read_csv('./utils/AppStore_Data.csv')\n    except ImportError:\n        raise ImportError(\n            \"pandas is required to use get_app_store_data(). Please install it first.\")\n</pre> def get_app_store_data():     try:         import pandas as pd         return pd.read_csv('./utils/AppStore_Data.csv')     except ImportError:         raise ImportError(             \"pandas is required to use get_app_store_data(). Please install it first.\") In\u00a0[\u00a0]: Copied! <pre>def get_embeddings(texts: list[str], model=\"text-embedding-3-large\"):\n    try:\n        from openai import OpenAI\n\n        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n        embeddings = client.embeddings.create(input=texts, model=model)\n        return [embedding.embedding for embedding in embeddings.data]\n    except ImportError:\n        raise ImportError(\n            \"openai is required to use get_embeddings(). Please install it first.\")\n    except Exception as e:\n        raise Exception(f\"Error in get_embeddings: {str(e)}\")\n</pre> def get_embeddings(texts: list[str], model=\"text-embedding-3-large\"):     try:         from openai import OpenAI          client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))          embeddings = client.embeddings.create(input=texts, model=model)         return [embedding.embedding for embedding in embeddings.data]     except ImportError:         raise ImportError(             \"openai is required to use get_embeddings(). Please install it first.\")     except Exception as e:         raise Exception(f\"Error in get_embeddings: {str(e)}\") <p>Util Functions</p> In\u00a0[\u00a0]: Copied! <pre>def count_tokens(text: str):\n    \"\"\"Count the number of tokens in a string\"\"\"\n    try:\n        from tiktoken import get_encoding\n        encoder = get_encoding(\"cl100k_base\")\n        return len(encoder.encode(text))\n    except ImportError:\n        raise ImportError(\n            \"tiktoken is required to use count_tokens(). Please install it first.\")\n</pre> def count_tokens(text: str):     \"\"\"Count the number of tokens in a string\"\"\"     try:         from tiktoken import get_encoding         encoder = get_encoding(\"cl100k_base\")         return len(encoder.encode(text))     except ImportError:         raise ImportError(             \"tiktoken is required to use count_tokens(). Please install it first.\") In\u00a0[\u00a0]: Copied! <pre>def row_to_string(row):\n    \"\"\"Convert a row to a string\"\"\"\n    try:\n        app_string = f\"\"\"App Name: {row[\"name\"]}\nSize: {row[\"size\"]} MB\nPrice: {row[\"price\"]} {row[\"currency\"]}\nRating Count: {row[\"rating_count_tot\"]}\nUser Rating: {row[\"user_rating\"]}\nVersion: {row[\"ver\"]}\nGenre: {row[\"prime_genre\"]}\nDescription: {row[\"app_desc\"]}\"\"\"\n        return app_string\n    except AttributeError:\n        raise AttributeError(\n            \"The row object doesn't have the expected attributes. Make sure you're passing a valid DataFrame row.\")\n</pre> def row_to_string(row):     \"\"\"Convert a row to a string\"\"\"     try:         app_string = f\"\"\"App Name: {row[\"name\"]} Size: {row[\"size\"]} MB Price: {row[\"price\"]} {row[\"currency\"]} Rating Count: {row[\"rating_count_tot\"]} User Rating: {row[\"user_rating\"]} Version: {row[\"ver\"]} Genre: {row[\"prime_genre\"]} Description: {row[\"app_desc\"]}\"\"\"         return app_string     except AttributeError:         raise AttributeError(             \"The row object doesn't have the expected attributes. Make sure you're passing a valid DataFrame row.\") In\u00a0[\u00a0]: Copied! <pre>def get_context(tokens: int, df, golden_df=None, shuffle=True):\n    \"\"\"Get the context for a given number of tokens from App Store Data df\"\"\"\n    try:\n        import pandas as pd\n        import random\n\n        # 1. If shuffle is true, shuffle the df\n        if shuffle:\n            df: pd.DataFrame = df.sample(frac=1).reset_index(drop=True)\n\n        # 2. Combine the golden_df and df if golden_df was provided\n        if golden_df is not None:\n            combined_df = pd.concat([golden_df, df], ignore_index=True)\n        else:\n            combined_df = df\n\n        app_strs: list[str] = []\n        new_df = pd.DataFrame()\n        delimiter = \"\\n================\\n\"\n\n        # 3. Get the string for each app, count tokens, and append if within threshold\n        for _, row in combined_df.iterrows():\n            row_str = row_to_string(row)\n            num_tokens = count_tokens(\n                f\"{delimiter.join(app_strs)}{delimiter}{row_str}\")\n            if num_tokens &lt; tokens:\n                app_strs.append(row_str)\n                new_df = pd.concat(\n                    [new_df, pd.DataFrame([row])], ignore_index=True)\n            else:\n                break\n\n        # 4. Shuffle the strings\n        if shuffle:\n            random.shuffle(app_strs)\n\n        return delimiter.join(app_strs), new_df\n\n    except ImportError:\n        raise ImportError(\n            \"pandas is required to use get_context(). Please install it first.\")\n</pre> def get_context(tokens: int, df, golden_df=None, shuffle=True):     \"\"\"Get the context for a given number of tokens from App Store Data df\"\"\"     try:         import pandas as pd         import random          # 1. If shuffle is true, shuffle the df         if shuffle:             df: pd.DataFrame = df.sample(frac=1).reset_index(drop=True)          # 2. Combine the golden_df and df if golden_df was provided         if golden_df is not None:             combined_df = pd.concat([golden_df, df], ignore_index=True)         else:             combined_df = df          app_strs: list[str] = []         new_df = pd.DataFrame()         delimiter = \"\\n================\\n\"          # 3. Get the string for each app, count tokens, and append if within threshold         for _, row in combined_df.iterrows():             row_str = row_to_string(row)             num_tokens = count_tokens(                 f\"{delimiter.join(app_strs)}{delimiter}{row_str}\")             if num_tokens &lt; tokens:                 app_strs.append(row_str)                 new_df = pd.concat(                     [new_df, pd.DataFrame([row])], ignore_index=True)             else:                 break          # 4. Shuffle the strings         if shuffle:             random.shuffle(app_strs)          return delimiter.join(app_strs), new_df      except ImportError:         raise ImportError(             \"pandas is required to use get_context(). Please install it first.\")"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/ai-applications/","title":"AI Applications","text":""},{"location":"blog/category/personal-finance/","title":"Personal Finance","text":""},{"location":"blog/category/technology-experiments/","title":"Technology Experiments","text":""},{"location":"blog/category/llms/","title":"LLMs","text":""},{"location":"blog/category/evals/","title":"Evals","text":""},{"location":"blog/category/gemini/","title":"Gemini","text":""},{"location":"blog/category/openai/","title":"OpenAI","text":""},{"location":"blog/category/langchain/","title":"LangChain","text":""},{"location":"blog/category/experiment/","title":"Experiment","text":""}]}