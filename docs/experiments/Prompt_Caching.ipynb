{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Caching\n",
    "\n",
    "Gonna load all of the app data in and ask it questions with follow ups\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU google-generativeai python-dotenv pandas anthropic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import caching\n",
    "import datetime\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from utils import get_app_store_data, get_context\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all of the app store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get app store data\n",
    "df = get_app_store_data()\n",
    "\n",
    "# Get 150,000 tokens of context\n",
    "app_data_str, app_df = get_context(150000, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Prompt and Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an App Store Data Analyzer. You should analyze the provided app store data and answer the user's questions.\n",
    "\n",
    "- Only use the App Store Data provided in your context.\n",
    "- Do not answer questions you are not confident in answering because the answer can't be found in the provided context.\n",
    "- Think through your answer slowly, step by step before providing the final answer.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(app_data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"What app has the most ratings?\",\n",
    "    \"What are the features for the app 'Online Head Ball'?\",\n",
    "    \"What is the most expensive app in the 'Games' category?\",\n",
    "    # \"Which app has the longest description in the app store?\",\n",
    "    # \"What is the average rating of all free apps?\",\n",
    "    # \"Identify any app that is paid and has fewer than 100 ratings.\",\n",
    "    # \"List all apps that are categorized under 'Games' and have more than 400,000 ratings.\",\n",
    "    # \"Which app has the lowest price in the app store?\",\n",
    "    # \"Find all apps that have a title starting with the letter 'A'.\",\n",
    "    # \"What is the total number of ratings for all apps in the 'Health & Fitness' category?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini says \n",
    "> The model doesn't make any distinction between cached tokens and regular input tokens. Cached content is simply a prefix to the prompt.\n",
    "\n",
    "That means we are pretty much only able to cache the system prompt. So you wouldnt be able to cache search results or function call responses. We cant cache tools. It does make sense though, in an agent the only thing that is static is the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cache with a 5 minute TTL\n",
    "cache = caching.CachedContent.create(\n",
    "    model='models/gemini-1.5-flash-001',\n",
    "    display_name='App Store Data',  # used to identify the cache\n",
    "    system_instruction=system_prompt,\n",
    "    contents=[app_data_str],\n",
    "    ttl=datetime.timedelta(minutes=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n",
    "\n",
    "def chat_gemini(query: str):\n",
    "    # Call the model\n",
    "    response = model.generate_content([(query)])\n",
    "\n",
    "    print(response.usage_metadata)\n",
    "    print(f\"Question: {example}\")\n",
    "    print(f\"Answer: {response.text}\")\n",
    "    print(\"\\n======\\n\")\n",
    "    return response\n",
    "\n",
    "for example in examples:\n",
    "    chat_gemini(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "\n",
    "Cache tools\n",
    "https://python.langchain.com/v0.2/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html#langchain_anthropic.chat_models.ChatAnthropic.bind_tools\n",
    "\n",
    "Normal Chat\n",
    "https://docs.anthropic.com/en/docs/quickstart#call-the-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Literal of models\n",
    "from typing import Literal\n",
    "\n",
    "models = Literal[\"claude-3-5-sonnet-20240620\", \"claude-3-haiku-20240307\", \"claude-3-opus-20240229\"]\n",
    "\n",
    "def chat_anthropic(query: str, model: models = \"claude-3-5-sonnet-20240620\"):\n",
    "    response = client.beta.prompt_caching.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1024,\n",
    "        system=[\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": app_data_str,\n",
    "                \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "            }\n",
    "        ],\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"Answer: {response}\")\n",
    "    print(\"\\n======\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in examples:\n",
    "    chat_anthropic(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, so the rate limits are insanely low. We get 40k tokens per minute with 1M tokens PER DAY. We really cant do anything with that so I think thats where we end things. It would be interesting to try it but it doesnt look like this would even be an option for our experiment without getting a custom account.\n",
    "\n",
    "The max tier gives us 50,000,000 tokens per day with 400k tokens per minute. Thats enough to do 2 experiments per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
