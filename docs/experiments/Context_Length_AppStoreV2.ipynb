{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Length Experiment\n",
    "> An experiment to test Gemini 1.5 Flash's ability to answer questions about up to 1 million tokens of context.\n",
    "\n",
    "Gemini 1.5 Flash, with its million-token context capability, represents a significant advancement in LLMs. This expanded context length opens up new possibilities for processing and understanding vast amounts of information. However, it also raises important questions about how effectively such models can utilize this extensive context in practical applications. How can we really know what to expect from an analysis of a 1 million token context?\n",
    "\n",
    "Traditionally, long-context models have been evaluated using [\"needle in a haystack\" tests](https://arize.com/blog-course/the-needle-in-a-haystack-test-evaluating-the-performance-of-llm-rag-systems/), where specific and usually irrelevant information is hidden within a large context to assess the model's retrieval capabilities. While valuable, these tests don't fully explore a model's ability to reason across and synthesize information from its entire context - a crucial skill for many real-world applications. They also lean on irrelevant needles, which I believe gives the LLM a crutch of parsing out anomalies in the text rather than understanding it. \n",
    "\n",
    "This study aims to address this gap by conducting a comprehensive evaluation of Gemini 1.5 Flash's question-answering capabilities across varying context lengths. Using a dataset derived from the Apple App Store, we design an experiment that systematically increases the context from 50,000 tokens to the full million-token capacity.\n",
    "\n",
    "Our primary objectives are to:\n",
    "\n",
    "1. Assess Gemini 1.5 Flash's performance in answering specific questions as the context length increases.\n",
    "2. Explore the practical implications of using such large context lengths in real-world scenarios.\n",
    "\n",
    "The experiment involves a set of questions about a curated set of apps, requiring the model to synthesize information from different parts of the context. By incrementally increasing the context length, we aim to understand not just the model's information retrieval capabilities, but its ability to reason across vast amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following to recreate this experiment:\n",
    "1. A Langsmith account and API Key\n",
    "2. A Google AI Studio API key\n",
    "3. An OpenAI API key\n",
    "\n",
    "Create a copy of the [.env.sample](.env.sample) file saved as `.env` and add your API keys.\n",
    "\n",
    "Install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU pandas tiktoken langchain langchain-openai langchain-google-genai matplotlib langsmith python-dotenv seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our experiment utilizes the [App Store Apple Data Set (10k apps)](https://www.kaggle.com/datasets/ramamet4/app-store-apple-data-set-10k-apps) from Kaggle. This dataset was chosen for its rich information about various apps, allowing for deterministic question generation and answer validation using pandas operations.\n",
    "\n",
    "We start by loading the necessary libraries and the two main CSV files from the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langsmith import Client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Client()\n",
    "\n",
    "app_data_df = pd.read_csv('./data/AppleStore.csv')\n",
    "descriptions_df = pd.read_csv('./data/appleStore_description.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is then merged into a single dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_app_df = pd.merge(app_data_df, descriptions_df, on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing\n",
    "\n",
    "To prepare the data for our experiment, we perform several cleaning and preprocessing steps:\n",
    "\n",
    "1. Select relevant columns and rename for clarity\n",
    "2. Convert app size from bytes to megabytes\n",
    "3. Remove special characters from app names\n",
    "4. Filter out rows with empty names or zero ratings\n",
    "5. Remove entries with non-Latin characters in the description\n",
    "\n",
    "Here's the code implementing these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>currency</th>\n",
       "      <th>price</th>\n",
       "      <th>rating_count_tot</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>ver</th>\n",
       "      <th>prime_genre</th>\n",
       "      <th>app_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6548</th>\n",
       "      <td>1134867821</td>\n",
       "      <td>NOT ALONE  Story of a bird</td>\n",
       "      <td>116.121094</td>\n",
       "      <td>USD</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Games</td>\n",
       "      <td>! Now on X'mas special sales (~2017 Jan. 3rd) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6751</th>\n",
       "      <td>1145500015</td>\n",
       "      <td>Drifty Chase</td>\n",
       "      <td>180.987305</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1631</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>Games</td>\n",
       "      <td>!! 2016 Very Big Indie Pitch finalist at PGCon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>823804745</td>\n",
       "      <td>Multiplayer Terraria edition</td>\n",
       "      <td>15.058594</td>\n",
       "      <td>USD</td>\n",
       "      <td>3.99</td>\n",
       "      <td>6981</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Games</td>\n",
       "      <td>!!! First and the only app which allows to pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>949876643</td>\n",
       "      <td>Lumyer  augmented reality camera effects</td>\n",
       "      <td>116.251953</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3896</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0.1</td>\n",
       "      <td>Photo &amp; Video</td>\n",
       "      <td>!!! NEW !!!  TAP EFFECTS\\nTry the new Tap Effe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5519</th>\n",
       "      <td>1086929344</td>\n",
       "      <td>Dancing with the Stars The Official Game</td>\n",
       "      <td>334.543945</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1098</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>Games</td>\n",
       "      <td>!!! Please note this app does not currently su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                      name        size  \\\n",
       "6548  1134867821                NOT ALONE  Story of a bird  116.121094   \n",
       "6751  1145500015                              Drifty Chase  180.987305   \n",
       "2493   823804745              Multiplayer Terraria edition   15.058594   \n",
       "3273   949876643  Lumyer  augmented reality camera effects  116.251953   \n",
       "5519  1086929344  Dancing with the Stars The Official Game  334.543945   \n",
       "\n",
       "     currency  price  rating_count_tot  user_rating    ver    prime_genre  \\\n",
       "6548      USD   2.99                 1          3.0    1.1          Games   \n",
       "6751      USD   0.00              1631          4.5    1.7          Games   \n",
       "2493      USD   3.99              6981          4.0    1.5          Games   \n",
       "3273      USD   0.00              3896          4.5  4.0.1  Photo & Video   \n",
       "5519      USD   0.00              1098          4.0    2.7          Games   \n",
       "\n",
       "                                               app_desc  \n",
       "6548  ! Now on X'mas special sales (~2017 Jan. 3rd) ...  \n",
       "6751  !! 2016 Very Big Indie Pitch finalist at PGCon...  \n",
       "2493  !!! First and the only app which allows to pla...  \n",
       "3273  !!! NEW !!!  TAP EFFECTS\\nTry the new Tap Effe...  \n",
       "5519  !!! Please note this app does not currently su...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = full_app_df[['id', 'track_name_x', 'size_bytes_x', 'currency',\n",
    "                      'price', 'rating_count_tot', 'user_rating', 'ver', 'prime_genre', 'app_desc']]\n",
    "\n",
    "new_df = new_df.rename(columns={'track_name_x': 'name', 'size_bytes_x': 'size'})\n",
    "\n",
    "new_df['size'] = new_df['size'] / (1024 * 1024)  # Convert to MB\n",
    "new_df['name'] = new_df['name'].str.replace(r\"[^a-zA-Z0-9\\s]+\", \"\", regex=True)\n",
    "new_df = new_df[new_df['name'].str.strip() != \"\"]\n",
    "new_df = new_df[new_df['rating_count_tot'] != 0]\n",
    "new_df = new_df[new_df['app_desc'].str.contains(r'[\\u4e00-\\u9fff]') == False]\n",
    "new_df = new_df.sort_values(by='app_desc')\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golden Dataset Selection\n",
    "\n",
    "To create a controlled subset for our questions, we select a \"golden dataset\" of five apps. Rather than random selection, which could introduce variability across experiments, we chose a deterministic approach:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>currency</th>\n",
       "      <th>price</th>\n",
       "      <th>rating_count_tot</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>ver</th>\n",
       "      <th>prime_genre</th>\n",
       "      <th>app_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>529997671</td>\n",
       "      <td>Disney Channel  Watch Full Episodes Movies  TV</td>\n",
       "      <td>125.921875</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21082</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.7.0</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>K.C. Undercover, Liv &amp; Maddie, Bunk’d and more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3455</th>\n",
       "      <td>965789238</td>\n",
       "      <td>1000</td>\n",
       "      <td>73.989258</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.6.5</td>\n",
       "      <td>Shopping</td>\n",
       "      <td>KAOLA.COM is China 's largest overseas commodi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>994674676</td>\n",
       "      <td>Sago Mini Superhero</td>\n",
       "      <td>171.169922</td>\n",
       "      <td>USD</td>\n",
       "      <td>2.99</td>\n",
       "      <td>30</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Education</td>\n",
       "      <td>KAPOW! Jack the rabbit bursts into the sky as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>645949180</td>\n",
       "      <td>Jelly Splash</td>\n",
       "      <td>132.311523</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21601</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.13.0</td>\n",
       "      <td>Games</td>\n",
       "      <td>KICK BACK AND SPLASH!\\n\\nJoin those delicious ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5076</th>\n",
       "      <td>1070850573</td>\n",
       "      <td>KQ MiniSynth</td>\n",
       "      <td>19.365234</td>\n",
       "      <td>USD</td>\n",
       "      <td>5.99</td>\n",
       "      <td>15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.7.4</td>\n",
       "      <td>Music</td>\n",
       "      <td>KQ MiniSynth is a polyphonic modular synthesiz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                            name        size  \\\n",
       "1350   529997671  Disney Channel  Watch Full Episodes Movies  TV  125.921875   \n",
       "3455   965789238                                            1000   73.989258   \n",
       "3793   994674676                             Sago Mini Superhero  171.169922   \n",
       "1965   645949180                                    Jelly Splash  132.311523   \n",
       "5076  1070850573                                    KQ MiniSynth   19.365234   \n",
       "\n",
       "     currency  price  rating_count_tot  user_rating     ver    prime_genre  \\\n",
       "1350      USD   0.00             21082          3.5   5.7.0  Entertainment   \n",
       "3455      USD   0.00                23          4.5   3.6.5       Shopping   \n",
       "3793      USD   2.99                30          3.5     1.1      Education   \n",
       "1965      USD   0.00             21601          4.0  3.13.0          Games   \n",
       "5076      USD   5.99                15          5.0   1.7.4          Music   \n",
       "\n",
       "                                               app_desc  \n",
       "1350  K.C. Undercover, Liv & Maddie, Bunk’d and more...  \n",
       "3455  KAOLA.COM is China 's largest overseas commodi...  \n",
       "3793  KAPOW! Jack the rabbit bursts into the sky as ...  \n",
       "1965  KICK BACK AND SPLASH!\\n\\nJoin those delicious ...  \n",
       "5076  KQ MiniSynth is a polyphonic modular synthesiz...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_df = new_df[new_df['app_desc'].str.startswith(('K', 'k'))].head(5)\n",
    "clean_df = new_df.drop(golden_df.index)\n",
    "golden_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This method selects the first five apps whose descriptions start with 'K' or 'k'. While somewhat arbitrary, this approach ensures:\n",
    "\n",
    "1. Reproducibility across experiments\n",
    "2. A degree of randomness to avoid bias\n",
    "\n",
    "By separating our golden dataset from the main dataframe, we can ensure these apps are always included in our context, regardless of the token limit, while filling the remaining context with other app data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Dataset\n",
    "\n",
    "Our evaluation dataset consists of three carefully crafted questions designed to test Gemini 1.5 Flash's ability to synthesize information across the App Store data context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Do the 'Sago Mini Superhero' and 'Disney Channel  Watch Full Episodes Movies  TV' apps require internet connection?\",\n",
    "        \"answer\": \"You can play Sago Mini Superhero without wi-fi or internet. Internet is required for Disney Channel  Watch Full Episodes Movies  TV\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Where can I find the privacy policy for the 'Disney Channel  Watch Full Episodes Movies  TV' app?\",\n",
    "        \"answer\": \"http://disneyprivacycenter.com/\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which one costs less? The 'KQ MiniSynth' app or the 'Sago Mini Superhero' app?\",\n",
    "        \"answer\": \"The 'KQ MiniSynth' app costs $5.99, the 'Sago Mini Superhero' app costs $2.99. So 'Sago Mini Superhero' is cheaper\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we upload them to Langsmith as a dataset if they haven't already been uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"AppStore Q&A\"\n",
    "\n",
    "\n",
    "def make_dataset():\n",
    "    \"\"\"Make and fill dataset if it doesnt exist\"\"\"\n",
    "    if client.has_dataset(dataset_name=dataset_name):\n",
    "        return client.read_dataset(dataset_name=dataset_name)\n",
    "\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name, description=\"App Store Data questions and answers\")\n",
    "\n",
    "    for example in examples:\n",
    "        client.create_example(\n",
    "            inputs={\"question\": example[\"question\"]}, outputs={\"answer\": example[\"answer\"]}, dataset_name=dataset.name)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "dataset = make_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our experiment is designed to test Gemini 1.5 Flash's performance across increasing context lengths. To execute this, we need several components:\n",
    "\n",
    "1. The App Store dataset\n",
    "2. Our evaluation dataset (Q&As)\n",
    "3. An evaluation function\n",
    "4. A prediction function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function\n",
    "\n",
    "The evaluation function is a critical component of our experiment. It's responsible for:\n",
    "\n",
    "1. Comparing the model's output to the correct answer\n",
    "2. Providing a detailed assessment of the answer's correctness\n",
    "3. Assigning a binary score (correct or incorrect)\n",
    "\n",
    "Key aspects of this implementation:\n",
    "\n",
    "1. **GPT-4o as Judge**: We use GPT-4o (via the `ChatOpenAI` class) to evaluate the answers. This allows for nuanced understanding and assessment of the responses. If you haven't worked with LLM's as a judge, it may sound unreliable to have an LLM grade an LLM but it is very reliable in scenarios like this. You can think about it like this: GPT-4o is more than capable of validating that an answer is the same as the correct answer we give it. This is a much more simple task than the one we are giving Gemini Flash of actually generating the correct answer.\n",
    "\n",
    "2. **Structured Output**: Using `with_structured_output(EvaluationSchema)` ensures that our evaluation consistently provides both reasoning and a binary correctness judgment that we can use programmatically.\n",
    "\n",
    "3. **Detailed Evaluation**: The system prompt instructs the GPT-4o model to provide thorough reasoning, considering partial correctness and nuances in the answers.\n",
    "\n",
    "4. **Binary Scoring**: While the evaluation includes detailed reasoning, the final score is binary (0 or 1) for simplicity in aggregating results across multiple questions and context lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "class EvaluationSchema(BaseModel):\n",
    "    \"\"\"An evaluation schema for assessing the correctness of an answer\"\"\"\n",
    "    reasoning: str = Field(\n",
    "        description=\"Detailed reasoning for the evaluation score\")\n",
    "    correct: bool = Field(\n",
    "        description=\"Whether the user's answer is correct or not\")\n",
    "\n",
    "def qa_eval(root_run: Run, example: Example):\n",
    "    \"\"\"Evaluate the correctness of an answer to a given question\"\"\"\n",
    "    question = example.inputs[\"question\"]\n",
    "    user_answer = root_run.outputs[\"output\"]\n",
    "    correct_answer = example.outputs[\"answer\"]\n",
    "\n",
    "    if not question or not user_answer or not correct_answer:\n",
    "        return {\n",
    "            \"score\": 0,\n",
    "            \"key\": \"correctness\",\n",
    "            \"comment\": \"Question, user's answer, or correct answer is missing\"\n",
    "        }\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\", temperature=0.4).with_structured_output(EvaluationSchema)\n",
    "\n",
    "    system_prompt = f\"\"\"You are a judge tasked with evaluating a user's answer to a given question. \n",
    "You will be provided with the question, the correct answer, and the user's thought process and answer.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Correct Answer:\n",
    "{correct_answer}\n",
    "\n",
    "Your job is to assess the user's answer and provide:\n",
    "1. Detailed reasoning for your evaluation, comparing the user's answer to the correct answer\n",
    "2. A boolean judgment on whether the user's answer is correct or not\n",
    "\n",
    "Be thorough in your reasoning and accurate in your judgment. Consider partial correctness and any nuances in the answers.\"\"\"\n",
    "\n",
    "    evaluation: EvaluationSchema = llm.invoke(\n",
    "        [SystemMessage(content=system_prompt),\n",
    "         HumanMessage(content=user_answer)]\n",
    "    )\n",
    "\n",
    "    score = 1 if evaluation.correct else 0\n",
    "\n",
    "    return {\n",
    "        \"score\": score,\n",
    "        \"key\": \"correctness\",\n",
    "        \"comment\": evaluation.reasoning\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This evaluation function allows us to assess Gemini 1.5 Flash's performance consistently across different context lengths and questions. By using a language model (GPT-4) as the judge, we can capture subtle aspects of correctness that might be missed by simpler, rule-based evaluation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prediction Function\n",
    "\n",
    "The prediction function is a crucial component of our experiment. It's responsible for:\n",
    "\n",
    "1. Taking a question from our evaluation dataset\n",
    "2. Generating a context of appropriate length\n",
    "3. Querying Gemini 1.5 Flash with the question and context\n",
    "4. Returning the model's response\n",
    "\n",
    "Here's our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken import get_encoding\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Gemini's 1M token limit\n",
    "max_context_limit = 1000000\n",
    "\n",
    "\n",
    "# Util Functions\n",
    "\n",
    "def count_tokens(text: str):\n",
    "    \"\"\"Count the number of tokens in a string\"\"\"\n",
    "    encoder = get_encoding(\"cl100k_base\")\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "\n",
    "def row_to_string(row):\n",
    "    \"\"\"Convert a row to a string\"\"\"\n",
    "    app_string = f\"\"\"App Name: {row.name}\n",
    "Size: {round(row.size, 2)} MB\n",
    "Price: {row.price} {row.currency}\n",
    "Rating Count: {row.rating_count_tot}\n",
    "User Rating: {row.user_rating}\n",
    "Version: {row.ver}\n",
    "Genre: {row.prime_genre}\n",
    "Description: {row.app_desc}\"\"\"\n",
    "    return app_string\n",
    "\n",
    "\n",
    "def get_context(tokens: int):\n",
    "    \"\"\"Get the context for a given number of tokens\"\"\"\n",
    "    # Combine the golden df and the new_df\n",
    "    combined_df = pd.concat([golden_df, new_df])\n",
    "    app_strs: list[str] = []\n",
    "    delimiter = \"\\n================\\n\"\n",
    "\n",
    "    for i, row in enumerate(combined_df.itertuples()):\n",
    "        row_str = row_to_string(row)\n",
    "        num_tokens = count_tokens(\n",
    "            f\"{delimiter.join(app_strs)}{delimiter}{row_str}\")\n",
    "        if num_tokens < tokens:  # If we havent hit the token limit, add the row\n",
    "            app_strs.append(row_str)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Randomize app strings\n",
    "    random.shuffle(app_strs)\n",
    "    return delimiter.join(app_strs)\n",
    "\n",
    "\n",
    "def visualize_test_results(experiments):\n",
    "    \"\"\"Display a graph of the test results\"\"\"\n",
    "    # Step 1: Extract and process data\n",
    "    all_results = []\n",
    "    for exp in experiments:\n",
    "        df = client.get_test_results(project_name=exp[\"results\"].experiment_name)\n",
    "        df['tokens'] = exp['tokens']\n",
    "        all_results.append(df)\n",
    "    \n",
    "    # Combine all results into a single dataframe\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Step 2: Sort by token count\n",
    "    combined_df = combined_df.sort_values('tokens')\n",
    "    \n",
    "    # Get unique questions\n",
    "    questions = combined_df['input.inputs.question'].unique()\n",
    "    \n",
    "    # Create a color palette\n",
    "    color_palette = sns.color_palette(\"husl\", n_colors=len(questions))\n",
    "    \n",
    "    # Step 3: Create the line graph\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        question_data = combined_df[combined_df['input.inputs.question'] == question]\n",
    "        ax.plot(question_data['tokens'], question_data['feedback.correctness'], \n",
    "                label=f'Question {i+1}', color=color_palette[i])\n",
    "    \n",
    "    ax.set_title('Test Results by Token Count')\n",
    "    ax.set_xlabel('Number of Tokens')\n",
    "    ax.set_ylabel('Correctness Score')\n",
    "    ax.legend(title='Questions', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    # Add questions as text below the graph\n",
    "    fig.text(0.1, 0.02, \"Questions:\", fontweight='bold')\n",
    "    for i, question in enumerate(questions):\n",
    "        fig.text(0.1, -0.02 - 0.03*i, f\"{i+1}. {question}\", fontsize=8, wrap=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.3)  # Adjust this value to fit all questions\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, step=1, total_steps=20, model=\"gemini-1.5-flash\"):\n",
    "        self.step = step\n",
    "        self.total_steps = total_steps\n",
    "        self.model = model\n",
    "        self.llm = ChatGoogleGenerativeAI(model=model)\n",
    "        self.experiments = []\n",
    "\n",
    "    def predict(self, inputs: dict):\n",
    "        \"\"\"Prediction function for Gemini Experiment\"\"\"\n",
    "        tokens = (max_context_limit / self.total_steps) * self.step\n",
    "        context = get_context(tokens)\n",
    "\n",
    "        system_prompt = f\"\"\"You are tasked with answering user questions based on the the App Store data inside <APP STORE DATA>.\n",
    "<APP STORE DATA> contains a ton of public data about apps on the App Store. It is the most current and accurate source \\\n",
    "so be sure to ONLY answer based on the context in <APP STORE DATA>. You will be graded on accuracy so be very careful and \\\n",
    "make sure you are as accurate as possible. First, think through your reasoning to answering the question before ultimately repeating \\\n",
    "the question and giving your answer.\n",
    "\n",
    "<APP STORE DATA>\n",
    "{context}\n",
    "</APP STORE DATA>\"\"\"\n",
    "        response = self.llm.invoke(\n",
    "            [SystemMessage(content=system_prompt), HumanMessage(content=inputs[\"question\"])])\n",
    "        return {\"output\": response.content}\n",
    "\n",
    "    def _run_eval(self):\n",
    "        \"\"\"Run a single evaluation for Gemini Experiment\"\"\"\n",
    "        tokens = (max_context_limit / self.total_steps) * self.step\n",
    "\n",
    "        result = evaluate(\n",
    "            self.predict,\n",
    "            data=client.list_examples(dataset_name=dataset_name),\n",
    "            evaluators=[qa_eval],\n",
    "            experiment_prefix=f\"{self.model}-{tokens}\"\n",
    "        )\n",
    "\n",
    "        # Append the results to the experiments list\n",
    "        self.experiments.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"step\": self.step,\n",
    "            \"results\": result\n",
    "        })\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run a single step of the Gemini Experiment\"\"\"\n",
    "        print(f\"Running step {self.step} of the Gemini Experiment\")\n",
    "        self._run_eval()\n",
    "        # Increment the step\n",
    "        self.step += 1\n",
    "        # If we have more than 1 experiment, display the results\n",
    "        if len(self.experiments) > 1:\n",
    "            visualize_test_results(self.experiments)\n",
    "\n",
    "    def run_all(self, reset=False, stop_at=None):\n",
    "        \"\"\"Run all steps of the Gemini Experiment\n",
    "\n",
    "        Args:\n",
    "            reset (bool, optional): Whether to reset the step counter. Defaults to False.\n",
    "            stop_at (int, optional): The step to stop at. Defaults to Predictor.total_steps.\n",
    "        \"\"\"\n",
    "        if stop_at is None:\n",
    "            stop_at = self.total_steps\n",
    "\n",
    "        if reset:\n",
    "            self.step = 1\n",
    "\n",
    "        while self.step <= stop_at:\n",
    "            self.run()\n",
    "\n",
    "\n",
    "eval = Predictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Key aspects of this implementation:\n",
    "\n",
    "1. **Context Generation**: The `get_context` function generates a context of the appropriate length for each step of the experiment.\n",
    "\n",
    "2. **Incremental Context**: The `step` and `total_steps` parameters allow us to incrementally increase the context length from 50,000 to 1,000,000 tokens. We have to use a class because our prediction function should only take the `inputs` dict.\n",
    "\n",
    "3. **System Prompt**: We wrote a system prompt to instruct Gemini 1.5 Flash on how to approach the task. This prompt emphasizes:\n",
    "   - Using only the provided context\n",
    "   - The importance of accuracy\n",
    "   - The need for reasoning before answering\n",
    "\n",
    "4. **Model Invocation**: We use the `ChatGoogleGenerativeAI` class from the `langchain_google_genai` library to interact with Gemini 1.5 Flash.\n",
    "\n",
    "This prediction function allows us to systematically test Gemini 1.5 Flash's performance across varying context lengths while maintaining consistent instructions and evaluation criteria. By incrementing the `step` parameter, we can observe how the model's performance changes as it has access to more context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets run our experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The results of our experiment with Gemini 1.5 Flash are remarkable in their consistency. Across all context lengths, from 50,000 tokens all the way up to the full million-token capacity, Gemini 1.5 Flash achieved 100% accuracy in answering our test questions!\n",
    "\n",
    "**[View Test Results on LangSmith](https://smith.langchain.com/public/0f86f6ab-aaf0-4262-b38d-bed96e243a15/d)**\n",
    "\n",
    "| question                                                                                                            |   50k |   100k |   150k |   200k |   250k |   300k |   350k |   400k |   450k |   500k |   550k |   600k |   950k |\n",
    "|:--------------------------------------------------------------------------------------------------------------------|------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|\n",
    "| Which one costs less? The 'KQ MiniSynth' app or the 'Sago Mini Superhero' app?                                      |     1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |\n",
    "| Where can I find the privacy policy for the 'Disney Channel  Watch Full Episodes Movies  TV' app?                   |     1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |\n",
    "| Do the 'Sago Mini Superhero' and 'Disney Channel  Watch Full Episodes Movies  TV' apps require internet connection? |     1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |      1 |\n",
    "\n",
    "\n",
    "Key Findings:\n",
    "\n",
    "1. **Perfect Accuracy**: Gemini 1.5 Flash maintained 100% correctness across all context lengths, from 50,000 to 1,000,000 tokens.\n",
    "2. **Scalability**: The model's performance did not degrade as the context length increased, demonstrating robust scalability.\n",
    "3. **Consistency**: Regardless of the amount of context provided, Gemini 1.5 Flash consistently provided accurate answers, indicating strong information synthesis capabilities.\n",
    "\n",
    "It's important to note that our experiment was carefully designed to avoid numerical reasoning and relational queries. Previous experiments have shown that Gemini Flash struggles with tasks involving logic around numbers, such as identifying \"the highest rated\" or \"Top 5 by size\" apps. Our questions focused on factual retrieval and simple comparisons, areas where the model excels.\n",
    "\n",
    "#### Implications:\n",
    "\n",
    "1. **Comprehensive Document Analysis**: \n",
    "   Organizations can now process entire documents or databases in a single query. For example, a company could input all its policy documents, employee handbooks, and project reports into Gemini 1.5 Flash. This would allow for quick and accurate answers to complex queries that span multiple documents, potentially saving hours of manual searching and cross-referencing.\n",
    "\n",
    "2. **Enhanced Customer Support**:\n",
    "   Customer service departments could leverage Gemini 1.5 Flash to create incredibly knowledgeable chatbots. By inputting all product information, past customer interactions, and frequently asked questions, these chatbots could provide accurate, context-aware responses to customer queries. This could significantly reduce response times and improve customer satisfaction while decreasing the workload on human customer service representatives.\n",
    "\n",
    "3. **Improved Contract Analysis**:\n",
    "   Legal departments and law firms could use Gemini 1.5 Flash to analyze lengthy contracts and legal documents. By inputting multiple related contracts, case law, and regulatory information, lawyers could quickly get accurate answers to specific legal questions, potentially speeding up contract review processes and reducing the risk of overlooking important clauses or legal precedents.\n",
    "\n",
    "While these results are extremely promising, it's crucial to remember that they are based on a specific dataset and set of questions. The model's performance on numerical reasoning and relational queries remains a limitation. Further testing across diverse domains and more complex query types would be beneficial to fully understand the capabilities and limitations of Gemini 1.5 Flash in real-world scenarios.\n",
    "\n",
    "Nevertheless, these results mark a significant leap forward in the field of large language models, particularly in handling and analyzing vast amounts of textual information. The ability to maintain perfect accuracy across such a large context opens up exciting possibilities for businesses dealing with large volumes of documents and data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
