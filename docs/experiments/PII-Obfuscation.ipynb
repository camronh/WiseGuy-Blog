{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PII Obfuscation\n",
        "\n",
        "This notebook demonstrates how PII (Personally Identifiable Information) obfuscation can work within the context of an LLM Agent.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The goal is to prevent the LLM from seeing any PII. We achieve this by following this flow:\n",
        "\n",
        "1. **Mask** the raw user message upon arrival.\n",
        "2. Pass the masked message (along with chat history) to the LLM.\n",
        "3. The LLM invokes a tool using PII tokens instead of actual PII.\n",
        "4. Inside the tool, access the vault to **unmask** the PII.\n",
        "5. Invoke the tool with real user data and get a response.\n",
        "6. **Mask** the response and return it to the LLM, allowing it to respond using tokens.\n",
        "7. **Unmask** the final response before displaying it to the user.\n",
        "\n",
        "---\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, install the required libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "%pip install -qU llm-guard langgraph langchain-core langchain-openai python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Load environment variables (e.g., API keys) from a `.env` file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables from a .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Initializing PII Vault and Scanner\n",
        "\n",
        "We use `llm-guard` to handle PII masking and vault management.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Vault and Anonymize classes from llm-guard\n",
        "from llm_guard.vault import Vault\n",
        "from llm_guard.input_scanners import Anonymize\n",
        "\n",
        "# Initialize a vault to store PII and create a scanner for PII anonymization\n",
        "vault = Vault()\n",
        "scanner = Anonymize(vault)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Example: Masking PII\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Mask PII in a sample string\n",
        "response = scanner.scan(\"Hi my email is johnsmith@gmail.com\")\n",
        "print(f\"Sanitized Prompt: {response[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The PII is stored in the vault\n",
        "scanner._vault.get()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Adding More PII to the Vault\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Subsequent scans will add new PII to the vault\n",
        "response = scanner.scan(\"My name is John Smith\")\n",
        "scanner._vault.get()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Unmasking Function\n",
        "\n",
        "We define a function to unmask text using the PII stored in the vault.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to unmask text using the PII stored in the vault\n",
        "def unmask(scanner: Anonymize, text: str):\n",
        "    # Retrieve the list of PII entities from the vault\n",
        "    entities = scanner._vault.get()\n",
        "\n",
        "    # Loop through the entities and replace the tokens with the original PII strings\n",
        "    for token, original_pii in entities:\n",
        "        text = text.replace(token, original_pii)\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Creating the Account Lookup Tool\n",
        "\n",
        "We create a mock account lookup function and wrap it with a `@tool` decorator for use in LangGraph.\n",
        "\n",
        "The tool:\n",
        "\n",
        "1. **Unmasks** the input arguments.\n",
        "2. Performs the account lookup (mocked).\n",
        "3. **Masks** the output.\n",
        "4. Returns the masked output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def account_lookup(masked_name: str, masked_email: str):\n",
        "    \"\"\"\n",
        "    Look up a user's account information based on their name and email.\n",
        "    Expects inputs to be masked PII tokens.\n",
        "\n",
        "    Parameters:\n",
        "    - masked_name (str): Masked token representing the user's name.\n",
        "    - masked_email (str): Masked token representing the user's email.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Masked account information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Unmask the arguments to get the real PII for account lookup\n",
        "    real_name = unmask(scanner, masked_name)\n",
        "    real_email = unmask(scanner, masked_email)\n",
        "\n",
        "    # Mock account lookup process\n",
        "    print(f\"Looking up account for {real_name} with email {real_email}\")\n",
        "\n",
        "    mock_account_data = {\n",
        "        \"name\": masked_name,\n",
        "        \"email\": masked_email,\n",
        "        \"username\": \"jsmith22\",\n",
        "        \"phone_number\": \"(555) 555-1234\",\n",
        "        \"address\": \"1234 Main St, Anytown, USA\",\n",
        "        \"account_balance\": \"$1,000.75\"\n",
        "    }\n",
        "\n",
        "    print(f\"Found account: {mock_account_data['username']}\")\n",
        "\n",
        "    # Mask any PII in the account data before returning\n",
        "    # Mask the dict by scanning its JSON string representation\n",
        "    masked_account_str = scanner.scan(json.dumps(mock_account_data))[0]\n",
        "    masked_account_data = json.loads(masked_account_str)\n",
        "\n",
        "    # Manually mask fields that may not be automatically masked\n",
        "    # For example, mask the username and address\n",
        "    scanner._vault.append((\"[REDACTED_USERNAME_1]\", mock_account_data[\"username\"]))\n",
        "    scanner._vault.append((\"[REDACTED_ADDRESS_1]\", mock_account_data[\"address\"]))\n",
        "    masked_account_data[\"username\"] = \"[REDACTED_USERNAME_1]\"\n",
        "    masked_account_data[\"address\"] = \"[REDACTED_ADDRESS_1]\"\n",
        "\n",
        "    # Return the masked account data to the LLM\n",
        "    return masked_account_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Building the Agent with LangGraph\n",
        "\n",
        "We use LangGraph to build the agent. The state only needs to track the `messages` since we access the vault via the `scanner` object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the LLM with the account_lookup tool bound to it\n",
        "llm_with_tools = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\", temperature=0).bind_tools([account_lookup])\n",
        "\n",
        "# Define the system prompt for the assistant\n",
        "system_prompt = \"\"\"You are a customer assistant agent. Your job is to look up the user's account information if they request it.\n",
        "The user's personal details are masked in the transcript and replaced with tokens (e.g., '[REDACTED_NAME_1]'). Use the tokens in the\n",
        "account lookup tool arguments when invoking it.\"\"\"\n",
        "\n",
        "# Define the agent's state structure\n",
        "class AgentState(TypedDict):\n",
        "    # We want to allow overwriting of messages so that we can mask and unmask them in pre and post processing\n",
        "    messages: list[BaseMessage]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Defining the Nodes\n",
        "\n",
        "We define the preprocessing, postprocessing, and model call nodes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# Preprocessing node: Mask PII in the messages before sending them to the LLM\n",
        "def pre_process(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Mask PII in the messages before sending them to the LLM.\n",
        "    \"\"\"\n",
        "    # Deep copy the messages to avoid modifying the original state\n",
        "    messages = copy.deepcopy(state[\"messages\"])\n",
        "    for message in messages:\n",
        "        # Replace the message content with the masked version\n",
        "        message.content = scanner.scan(message.content)[0]\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "# Postprocessing node: Unmask PII in the messages before returning them to the user\n",
        "def post_process(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Unmask PII in the messages before returning them to the user.\n",
        "    \"\"\"\n",
        "    # Deep copy the messages to avoid modifying the original state\n",
        "    messages = copy.deepcopy(state[\"messages\"])\n",
        "    for message in messages:\n",
        "        # Replace the message content with the unmasked version\n",
        "        message.content = unmask(scanner, message.content)\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "# Model call node: Invoke the LLM with the masked messages\n",
        "def call_model(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Call the LLM with the masked messages.\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    # Add the system prompt to the beginning of the messages array\n",
        "    system_message = SystemMessage(content=system_prompt)\n",
        "    # Invoke the LLM\n",
        "    response = llm_with_tools.invoke([system_message] + messages)\n",
        "    # Return the messages including the LLM's response\n",
        "    return {\"messages\": messages + [response]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Custom Tool Node\n",
        "\n",
        "We need to customize the tool node to return the full messages array because we overwrite messages returned by nodes. The prebuilt `ToolNode` only returns the tool messages. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the custom tool node\n",
        "class ToolNode:\n",
        "    \"\"\"\n",
        "    A node that runs the tools requested in the last AIMessage.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tools: list) -> None:\n",
        "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
        "\n",
        "    def __call__(self, inputs: dict):\n",
        "        # Retrieve the messages from the inputs\n",
        "        if messages := inputs.get(\"messages\", []):\n",
        "            # Get the last message (from the AI)\n",
        "            message = messages[-1]\n",
        "        else:\n",
        "            raise ValueError(\"No message found in input\")\n",
        "\n",
        "        outputs = []\n",
        "        # Process each tool call in the message\n",
        "        for tool_call in message.tool_calls:\n",
        "            # Invoke the tool with the provided arguments\n",
        "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
        "                tool_call[\"args\"]\n",
        "            )\n",
        "            # Create a ToolMessage with the result\n",
        "            outputs.append(\n",
        "                ToolMessage(\n",
        "                    content=json.dumps(tool_result),\n",
        "                    name=tool_call[\"name\"],\n",
        "                    tool_call_id=tool_call[\"id\"],\n",
        "                )\n",
        "            )\n",
        "        # Return the messages including the tool responses\n",
        "        return {\"messages\": messages + outputs}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Conditional Routing\n",
        "\n",
        "We need to route to the appropriate node based on whether the LLM's output is a tool invocation or a content response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_llm_output(state: AgentState) -> str:\n",
        "    \"\"\"\n",
        "    Determine the next node based on the LLM's output.\n",
        "\n",
        "    Returns:\n",
        "    - \"tool\" if the LLM invoked a tool.\n",
        "    - \"end\" if the LLM produced a content response.\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    if last_message.tool_calls:\n",
        "        return \"tool\"\n",
        "    else:\n",
        "        return \"end\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Building the Graph\n",
        "\n",
        "We assemble the nodes and define the edges to build the agent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph\n",
        "\n",
        "graph_builder = StateGraph(AgentState)\n",
        "\n",
        "# Add the custom nodes\n",
        "graph_builder.add_node(\"Preprocess\", pre_process)\n",
        "graph_builder.add_node(\"Call Model\", call_model)\n",
        "graph_builder.add_node(\"Post Process\", post_process)\n",
        "\n",
        "# Add the custom tool node\n",
        "graph_builder.add_node(\"Tool Call\", ToolNode(tools=[account_lookup]))\n",
        "\n",
        "# Define the edges\n",
        "# After preprocessing, call the model\n",
        "graph_builder.add_edge(\"Preprocess\", \"Call Model\")\n",
        "\n",
        "# After the model call, route based on the LLM's output\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"Call Model\",\n",
        "    route_llm_output,\n",
        "    {\"tool\": \"Tool Call\", \"end\": \"Post Process\"}\n",
        ")\n",
        "\n",
        "# After tool calls, return to the model call node\n",
        "graph_builder.add_edge(\"Tool Call\", \"Call Model\")\n",
        "\n",
        "# Set entry and finish points\n",
        "graph_builder.set_entry_point(\"Preprocess\")\n",
        "graph_builder.set_finish_point(\"Post Process\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Testing the Agent\n",
        "\n",
        "We instantiate the agent and test it with sample messages containing PII.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a new vault and scanner for a fresh session\n",
        "vault = Vault()\n",
        "scanner = Anonymize(vault)\n",
        "\n",
        "# Compile the graph to create the agent\n",
        "app = graph_builder.compile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### First User Message\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate a user message containing PII\n",
        "output: AgentState = app.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Hi my email is johnsmith@gmail.com\")]})\n",
        "\n",
        "# Print the output state\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Second User Message\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate a follow-up user message containing PII\n",
        "output: AgentState = app.invoke(\n",
        "    {\"messages\": output[\"messages\"] + [HumanMessage(content=\"Yea my name is John Smith. What is my address and account balance?\")]})\n",
        "\n",
        "# Print the output state\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Displaying the Conversation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the conversation messages in a readable format\n",
        "for message in output[\"messages\"]:\n",
        "    message.pretty_print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "In the resulting trace, we can see that the PII was masked internally at each step before finally being unmasked for the output.\n",
        "\n",
        "[View the trace](https://smith.langchain.com/public/c7c0d9e1-f814-4a07-b44d-cd5abe27d997/r)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "custom": {
      "cells": [],
      "metadata": {
        "language_info": {
          "name": "python"
        }
      },
      "nbformat": 4,
      "nbformat_minor": 2
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
